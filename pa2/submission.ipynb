{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "#plot X and y values\n",
    "X, y = load_dataset(\"toydata_large.csv\")\n",
    "plt.scatter(X[:, 2], X[:, 3], c=y, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Toy Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 values of y:  [1 1 0 1 1 1 0 0 1 0]\n",
      "First 10 values of X:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
      " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])\n",
      " list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 71690, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 74170, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 77842, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901])\n",
      " list([1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32])\n",
      " list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 15455, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 25242, 24762, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 16126, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 40732, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110])\n",
      " list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 15201, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 13209, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])]\n",
      "Shape of x_train:  (1000, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAABMfUlEQVR4nO3dd3xV5f3A8c/33JUJYa8AYQ+RLcst7lHcs646flptq7a2aocdamutdbZaV511L5y4tyAgQzZhB5JAIDu58zy/P+4hJOTmZpDkJuT7fr3yyr3PGfd7D5f7zTPO84gxBqWUUqoprEQHoJRSqv3SJKKUUqrJNIkopZRqMk0iSimlmkyTiFJKqSbTJKKUUqrJNIkopZRqMk0iSlUjIhtF5Gjn8SUiYkTknr32meWUP+k8z3Kelzk/+SLytogcE+Pclc4+hSLyjoj0jxPLZyLiF5FSESkRkYUicpOI+BrxfoyIDG3URVCqETSJKBXfOuBsEXFXK7sYWBNj3wxjTBowDvgQeF1ELtlrn1OcffoA+cAD9bz+tcaYdGf/XwLnAu+KiDT6nSjVAjSJKBVfHvADcByAiHQFZgCz6zrAGJNnjLkP+CNwp4jU+n9mjPEDrwCjGxKEMabcGPMZ8CNgOnCSE88UEflWRIpEJFdEHhQRr7PtC+fwJU7t5xwR6eLUknY4taG3RSSzITEoFYsmEaXq9zRwkfP4XOBNINCA414DegIj9t4gIinAOcDcxgRijNkMLAAOdYoiwPVAd6LJZSbwU2ffw5x9xhlj0owxLxL9P/9fYCAwAKgEHmxMDEpVp0lEqfq9DhwhIp2JJpOnG3jcNud312plb4hIEVAMHAPc1YR4tu0+pzFmoTFmrjEmbIzZCPwHOLyuA40xO40xrxpjKowxpcDt8fZXqj6aRJSqhzGmEngH+B3QzRjzdQMP7ef83lWt7FRjTAaQBFwLfC4ivRsZUr/d5xSR4U6TVJ6IlAB3EK2VxCQiKSLyHxHZ5Oz/BZAhIq5GxqAUoElEqYZ6mmjH9rONOOY0YDuweu8NxpiIMeY1os1RhzT0hM5orknAl07RQ8AqYJgxphNwCxCv0/2XRJvXpjr7727y0o561STu+ndRSgGfE21+WlTfjiLSCzgLuBX4hTHGjrGPEO0k7wKsbMA5U4CDgHuA74B3nU3pQAlQJiIjgauBHdUOzQcGA9nV9q8EipxBArfW99pKxaM1EaUawER9bIzZFWe3IhEpJzqa60TgLGPME3vt85aIlBH94r8duNgYszzOOR8UkVKiyeBe4FXg+GqJ6VfA+UAp8Cjw4l7H/xF4yhm9dbZzjmSggGin/vtxXlupeokuSqWUUqqptCailFKqyTSJKKWUajJNIkoppZpMk4hSSqkm63BDfLt3726ysrISHYZSSrUbCxcuLDDG9Ii1rcMlkaysLBYsWJDoMJRSqt0QkU11bdPmLKWUUk2mSUQppVSTaRJRSinVZJpElFJKNZkmkQaIVFSy66sFlC5fi04To5RSe3S40VmNtenxl1j5yzsQlwsTjpCclcmUtx4heUDfRIemlFIJpzWROArnLmb59bcTKa8kXFJGpKKS0pXrmHfCT7RGopRSaBKJa829TxGprLmUthibko25lCxakaColFKq7dAkEse2FVuxMNhisSOtDyW+DADCtrBtbV5ig1NKqTZA+0Ti2NRvPMFCDx+MPh/bcmOLRa+SzfxoyeNs7zKYUYkOUCnVaPk7/OzYGWBgZirpafoVuK/0CsZRdviJfCQHEXZ5q8q2dR7EUzNu4V9DuycwMqVUY1VURrj17ytYuKQIj0cIhQ3nzMrkyguziK5WrJpCm7PiCFpewi5PjTJjuaj0paP96kq1L3c+sJoFSwoJhmzKKyIEgzYvz87h/U/yEx1au6ZJJI6SkhBQ+y+U5CQXOwuDrR+QUqpJKirCfDG3gFCo5l9//oDN/17bkqCo9g+aROKYMqErPm/tSxQO24wYkp6AiJRSTVFeGcGqo8mqqCTUytHsXzSJxPGj4/uQ0dmDx73nw5fkszjzlH507eKNc6RSqi3p1sVLWmrtLmDLgkljM1o/oP2IJpE40lLdPHHvJM6elcmAfskcMCKdm38xgqsvGZzo0JRSjWBZwq9+Ogyfz2J3hcTtFlKS3Vxx4aDEBtfOSUe783ry5MlGF6VSqmNauaaE517dQk5uJePHdOa80/rTq0dSosNq80RkoTFmcqxtOsRXKdVhjBreidtuPiDRYexXtDlLKaVUk2kSUUop1WSaRBrIGEPZqnW6pohSqpbA9p0svep3fNhnGh9nHcaaPz9AJNAx7iXTPpEGKF68koVnX0swfycIeDI6MfH5++gyfUKTz1m6fC2bHv4flTl59Dj+MDJ/PAt3akozRq2Uag3h8gq+mnYGgbwdmFAYgHX/eJTCuYuZ+u7jCY6u5WkSqUe4vIK5x1xEuKikqixSXsm8Ey/jqHWf4O2a0ehz5r7+AYsvvhE7GIJIhIJPvmXjfU9y8NxX8XRKa8bolVItbevzbxHaVVSVQADsygCFXy+keNEKOk8YncDoWp42Z9Uj7/UPMOFwrXITibDthXcafT47FGLplb/FrvRDJBItq6ikcksuGx94ap/jVUq1rqK5i4mUV9YqN0DJ4v1/3aEWTSIicr2ILBeRZSLyvIgkicggEZknItki8qKIeJ19fc7zbGd7VrXz3OyUrxaR46qVH++UZYvITS3xHgLbd2LHaNu0K/0E8rY3+nyly9ZinORR43z+ALmvzmlSjEqpxEkdPggryVerXCyL5Kx+CYiodbVYEhGRfsDPgcnGmDGACzgXuBO4xxgzFCgELnMOuQwodMrvcfZDREY7xx0AHA/8W0RcIuIC/gWcAIwGznP2bVZdZ0zC8npqlbvSUuh66EGNPp87PRUTrp1EANzalKVUu9P/kjOwPDV7BsTtJqlPD7odPjVBUbWelm7OcgPJIuIGUoBc4CjgFWf7U8CpzuNZznOc7TMlOsn/LOAFY0zAGLMByAamOD/Zxpj1xpgg8IKzb7PKmDqObkdMw5WSXFXmSkmi84TRdJ85o9HnSx06kLRhWdFJe6pxpSYz6NoL9zVcpVQr8/XsxrRPniN97AjE40Y8brodOY1pnzyLWPt/j0GLdawbY7aKyD+AzUAl8AGwECgyxuzuZMgBdtf3+gFbnGPDIlIMdHPK51Y7dfVjtuxVHjPti8iVwJUAAwYMaNT7EBEmvfIgW554mS2Pv4KJRMi86DQG/N95Tf6ATHrt38w79hIC2wtABBMKMeCKc+l9xvFNOp9SKrE6jx/FYQtnEyosRjxu3GmpiQ6p1bRYEhGRLkRrBoOAIuBlos1Rrc4Y8wjwCETnzmrs8ZbbzcArz2Pglec1SzwpA/txxKoPKPzmewLbd9Jl6jiS+vZqlnMrpRLH06VzokNodS05xPdoYIMxZgeAiLwGHAxkiIjbqY1kAlud/bcC/YEcp/mrM7CzWvlu1Y+pq7zNExG6Hjwp0WEopdQ+ackGu83ANBFJcfo2ZgIrgE+BM519LgbedB7Pdp7jbP/ERG8Nnw2c64zeGgQMA74D5gPDnNFeXqKd77Nb8P0opZTaS0v2icwTkVeA74EwsIhok9I7wAsicptTtvuWzseBZ0QkG9hFNClgjFkuIi8RTUBh4BpjTARARK4F5hAd+fWEMWZ5S72f9sCftwN/Th5pIwbhTteRXkqplqfriewHIhWVLL7k12x/9zMsnxc7FGbwLy9j+B9+htSxJKhSSjVUvPVE9v/xZx3AD9fcyvb3PscOBAmXlGFX+tnwzyfIefr1RIemlNrPaRJp5yIVleS+/B62P1CrfP3djyUoKqVUR6FJpJ0LlZTVuS24Y1crRqKU6og0ibRzvp7dYo9Ntyy6Hjal9QNSSnUomkTaObEsDnjgVlwpSbC7E93twp2Wwoi/XJ/Y4JRS+z1NIg0QCQQpnLeEslXrEh1KTH1OPYapc56i1ylHkTZqCAMuOYNDF7xB2vBBiQ5NKbWf00Wp6rH1+bdYds2t0TmuwmGSs/pz0JsPk5KVmejQaugybTyTX/13osNQSnUwWhOJo3jxSpb+3+8Il5YTLikjUuGnbNU65h13qa6zrpRSaBKJa9NDz9ZekMq2CWwvoGjeksQEpZRSbYgmkTj8W7eDbdcqF8siuGNnAiJSSqm2RZNIHD1POgIrJalWuR0MkjF1fOsHpJRSbYwmkTgyLzqN5Mw+NdZPdqUmM/iXl+Pr2S2BkSmlVNugo7PicKemcMjcV9j40P/Ie20OnoxOZF17Ib1OPirRoSmlVJugSaQels9LUt+eJA/oi7dnN5L79010SEop1WZoEokjEgjy7ZEXULZiLZHySsRlkfP064x54Fb6X3x6osNTSqmE0z6ROHKeeZ3S5dEEAmAiNnaln+U//zPh8ooER6eUUomnSSSO3Jfexa6orFUubhdFcxe3fkBKKdXGaBKJw5PRKWa5sW1cqSmtHI1SSrU9mkTiGHjV+bhSkmuVezI6kTFlbAIiUkqptkWTSBzdj5rOkF9fgZXkxZ2eiis9FV/v7kx5+zHE0kunlFLS0SYSnDx5slmwYEGjjgnkF7Dry/l4umbQ7fApiMvVQtEppVTbIyILjTGTY23TIb4N4OvVnT5nnpDoMJRSqs3RNhmllFJNpjWRZrIqu5TnX9tCTm4lEw7M4NxZmXTv5qOoOMRbH2xjVXYZwwalMev4PnTJ8CY6XKX2OyU/rKZo/lKSM3vTfeYMbXZuJdon0gy+nFvAn/6xkkDQxhjwuIXkJBe333IAt9yxnEAwQjBo8HoFr8fFw3eNJ6t/arPGoFRHZYfDLDr/era//wUiAi4LT0Ynpn/6HCkD+yU6vP1CvD4Rbc7aR7ZtuOtfa/AHogkEIBQ2lFWE+dM/VlJWHiYYjG4IBg3lFWH++VB2AiNWav+y6aH/sX3OF9iVfiIVlURKy/Fvzef7865LdGgdgiaRfbRjZ4DyikitctuGgl1B9q7oGQOLlhVh2x2rBqhUS9n86AvYFf6ahbZN6dJV+PN2JCaoDkSTyD5KTXFj19EkKBL7GI/HqnObUqpxIpWB2BssC9tfxzbVbDSJ7KO0VDfTJ3fF46mZFZJ8FpPHZeD11LzEHo9w3BE9o223Sql91vesE7B8tQer+Hp1I1n7RFqcJpFm8NvrRjJudGe8XovUFBder8WZp/TjjlsOYMzITvh8FinJLpJ8FqOGpfOzy4cmOmSl9htDfvN/JA/sVzWfneXz4kpLYfxT/9A/1lqBjs5qRrn5frYX+Bk0MJVOaZ6q8uwNZWzYXMHAzGSGD0lvkddWqiOL+APkvvIeOz+fR8qg/vS/9EyS+vRMdFj7jXijszSJKKWUikuH+CqllGoRmkSUUko1mSYRpZRSTaZJRCmlVJO1aBIRkQwReUVEVonIShGZLiJdReRDEVnr/O7i7Csicr+IZIvIUhGZWO08Fzv7rxWRi6uVTxKRH5xj7pdWHs9njCH/rY/5btb/MfeYi9j8+MvYwWBrhqCUUgnV0jWR+4D3jTEjgXHASuAm4GNjzDDgY+c5wAnAMOfnSuAhABHpCtwKTAWmALfuTjzOPldUO+74Fn4/Naz45R0suvCX7Hj3M3Z+No8VN9zOvOMvxURqT4OilFL7oxZLIiLSGTgMeBzAGBM0xhQBs4CnnN2eAk51Hs8CnjZRc4EMEekDHAd8aIzZZYwpBD4Ejne2dTLGzDXRccpPVztXs1q3sYznX9/C7Dm5lJSGAFj04QrWP/Q8kfLKqv0iFZUUf7+C/Lc/bYkwlFKqzWnJ9UQGATuA/4rIOGAh8AuglzEm19knD+jlPO4HbKl2fI5TFq88J0Z5LSJyJdHaDQMGDGjwGzDGcPdDa3nvk3zsiMHlEu5/NJvTTujD6off5Ehb2HuyhUh5Bdvf+4zes45u8OsopVR71ZLNWW5gIvCQMWYCUM6episAnBpEi9/taIx5xBgz2RgzuUePHg0+bu7CXbz/aT6BgE0obPAHbPwBm+ff2EqpJGOk9uUTjxtvz+7NGb5SSrVZLZlEcoAcY8w85/krRJNKvtMUhfN7u7N9K9C/2vGZTlm88swY5c3mvY/z8fvtmNvW9xiDbdVeOU3cbvpffHpzhqGUUm1WiyURY0wesEVERjhFM4EVwGxg9wiri4E3ncezgYucUVrTgGKn2WsOcKyIdHE61I8F5jjbSkRkmjMq66Jq52oW8db8iFgenj/oekp9nQm4kvC7kwh7kxj/1F2kDml4k5lSSrVnLb3G+s+A50TEC6wHLiWauF4SkcuATcDZzr7vAicC2UCFsy/GmF0i8hdgvrPfn40xu5zHPwWeBJKB95yfZnPckb2Y+/2uWrUREbAsyO80gAePuJO+xRtJdkW48rYT6XNI3+YMQSml2jSdgDEOYwx/vnsVX84tIBC08bgFEeHGa4bx6tvbWL+5HLdLCIUNl543kAvP1BqIUmr/o7P4VtPYWXyNMSxfXcK8hbtITXVz9KE96d7NB8DmnAp2FgYZPiSN1JSWrtQppVRixEsi+s1XDxFhzMjOjBnZuda2AZkpDMhMSUBUSinVNujcWUoppZpMayLtTDhs8+b7ubz9YS7GwIkze3PaiX3xePTvAaVU69Mk0o4YY/j1n5exdEUx/kB0xNh/ntnAl/MKuP/2cbqetFKq1emfr+3I0hXFLF25J4EABAI2q9aWsmBJUeICU0p1WJpE6hEqKWPt3x7mq2ln8N1Jl7P9/c8TFsvSFSWEgrXvoK/02yxdUZyAiJRSHZ02Z8URLivnq6mn48/Jw/YHANj19QKG/Ob/GHbz1a0eT7cuXrxei8q9bn5M8ll067L3VJBKKdXy4tZERKSTiAyJUT625UJqO7Y88QqVW/ckEIBIeSXZdzxEcGdhq8dzxIzuuFy1+z0sS5h5aM9Wj0cppepMIiJyNrAKeFVElovIQdU2P9nSgbUFeW9/gqkM1Co3xlA0f2mrx5OS4ub+28fRp1cSST6LJJ9Frx4+7rttLOlpWqlUSrW+eN88twCTjDG5IjIFeEZEbjbGvA50iGFAdY12MoFgdPKsBBg+JJ2XHp3C5pxKDIaBmSk6KksplTDxkohr9+JRxpjvRORI4G0R6U8rrAHSFrhSk+vcFsgraMVIahIRBvbXO+WVUokX78/p0ur9IU5COYLoMrYHtHBcbUL1vpDaG3UddaWUilcTuZq9mq2MMaUicjx7pm/fr9mBUJ3bXCl111KUUqqjqLMmYoxZYozJjlEeMsY817JhtQ0pg/tHFw8BbKSqDc9KTsLXS5fAVUopHdITx8ArzmXJ2wt5d/BZbO0yBMtEGLVtPicXfkTXQ2LOiqyUUh2KJpE4zPCRPDvtJipCgFhExGJl34MIzzick12111dXSqmOpkHjVEUkudpa6R3G7Dm5hC0XyJ7LFLE8bNweYVV2aQIjU0qptqHeJCIipwCLgfed5+NFZHYLx9UmZG8oIxisPZpZRNicU5GAiJRSqm1pSE3kj8AUoAjAGLMYGNRiEbUhI4d1wuetfYls2zB4YGoCIlJKqbalIUkkZIzZe4rYDnGz4SnH9sbns6h+Q7jXI4wZ2Ymhg9ISF5hSSrURDUkiy0XkfMAlIsNE5AHgmxaOq03olO7h0X9OZMZB3fB6LNJSXZx2Yl/+/vsxzf5aO7+cz5dTT+fd1DF8nHUYmx7+H8Z0iFytlGrHpL4vKhFJAX4LHOsUzQFuM8b4Wzi2FjF58mSzYMGCRIdRQ+G8Jcw99iLsij2X1JWSzJCbEjPlvFJKVSciC40xMe9rqG8qeBfwjjHmt8aYg5yf37XXBNJWrfnjfTUSCECkopJ1f3+ESCCYoKiUUqp+cZOIMSYC2CLSuZXi6ZBKl62JvcEYArnbWzcYpZRqhIbcbFgG/CAiHwLluwuNMT9vsag6mNRhWQTydtTeYIxOr6KUatMakkRec35UCxn+x5/z3clXYFfW7BPJuubHuJKTEhiZUkrFV28SMcY81RqBtFXb8ip56Kn1LFhcRGqKizNPyeSsU/rFXKa2qbodNoWJL9zHihtup2L9Ftyd0hh8/aUM1U51pVQb15DRWRuIcV+IMWZwSwXVkhozOmtXYZAf/3Q+ZRVhbDtaluSzOPqwntz085aZBcYOhRC3W1crVEq1GfFGZzWkOav6gUnAWUDX5gisrXvl7a1UBiJVCQTAH7D54LN8Lr8gi+7dfM3+mpbH0+znVEqpltKQ5qydexXdKyILgT+0TEhtx9IVxYRCtWtqliVkbyjDNvD90iLSUl1MmdgVrycx664rpVSi1JtERGRitacW0ZpJh5hCfmBmCktXFNeoiUC0NnLrXSsJhmzcrui0KG6XcM9fxjJiaHpiglVKqQRoSDK4u9rjMLCBDrI87jmzMpnzaT7+gF1rW3lFdI31UGjPWuu/+tMPvPHk9GbtdFdKqbasIe0vlxljjnR+jjHGXAl0iNuoB2SmcNetBzZ4/0DA5oeVe89VqZRS+6+GJJFXGli2X9q0teHrhohAIFi71qKUUvurOpOIiIwUkTOAziJyerWfS4iO0moQEXGJyCIRedt5PkhE5olItoi8KCJep9znPM92tmdVO8fNTvlqETmuWvnxTlm2iNzU+Lcf3/LVJdzz8NoG7x+JGMaO1hlilFIdR7yayAjgZCADOKXaz0Tgika8xi+AldWe3wncY4wZChQClznllwGFTvk9zn6IyGjgXOAA4Hjg305icgH/Ak4ARgPnOfs2m6df2kQkEnubAD5f9PK5LPB5LX7102EkJ+na60qpjqPOjnVjzJvAmyIy3RjzbVNOLiKZwEnA7cANEr2D7ijgfGeXp4iunPgQMMt5DNHmsged/WcBLxhjAsAGEckmutIiQLYxZr3zWi84+65oSqyxrN9Ud1PWaSf1ZdK4Lnw1r4DO6R5OPrY3Wf11tUOlVMfSkNFZi0TkGqI1gapmLGPMTxpw7L3Ar4Hd4167AUXGmLDzPAfo5zzuB2xxzh0WkWJn/37A3GrnrH7Mlr3KpzYgpgbr3dNHbn7sWe/79k7i8OndOXy6TpColOq4GtKx/gzQGzgO+BzIBErrO0hETga2G2MW7lOEzUBErhSRBSKyYMeOGLPl1uHsH2USeyVgw9SJHeKmfaWUiqshSWSoMeb3QLkzGeNJNOwv/oOBH4nIRuAFos1Y9wEZIrK7BpQJbHUebwX6AzjbOwM7q5fvdUxd5bUYYx4xxkw2xkzu0aNHA0KPKlq6imH5i3HZod0nAmODgUt+vpB7Hl6ro7GUUh1aQ5KI8w1KkYiMIfrl3rO+g4wxNxtjMo0xWUQ7xj8xxlwAfAqc6ex2MfCm83i28xxn+ycmOjvkbOBcZ/TWIGAY8B0wHxjmjPbyOq8xuwHvp8FyHnqOUxc/wmmLHmZY/iIsY4NYIEIkYnj7wzxu/XuzdcEopVS705Ak8oiIdAF+T/RLegXw9314zd8Q7WTPJtrn8bhT/jjQzSm/AbgJwBizHHjJed33gWuMMRGnX+Vaomu+rwRecvZtNmml+bhNhGE7fiDdX8TeTVuBoM13iwrr7DdRSqn9XUMmYHzMefg50KTp340xnwGfOY/Xs2d0VfV9/ERnCI51/O1ER3jtXf4u8G5TYmqIzOMPpvjRNXjsENvTM7Gt2pfL6xZytlXQp5cuHqWU6njqrYmISC8ReVxE3nOejxaRy+o7bn9w8J8vg9Q0wuKiT/FGrN19I9UEQzYDMlMSEJ1SSiVeQ5qzniTaZNTXeb4GuK6F4mlTvF0zOH7FW/S44HTGF36Py9hUb9Ly+SwOnd6dXj20FqKU6pgakkS6G2NeAmyI3sMB1HEf9/4nqXcPZvz3Ni7e+i6PP3Qwk8Z2we0SXC4hELD5el4B/3x4LX5/h7kkSilVpSFJpFxEuuH8CS4i04AOOVXt4IGp3PqrUfh8FrYdrZH4A4a3P8jjptuWJTg6pZRqfQ25Y/0GoqOyhojI10AP9gzR7XBef28boZCh+tL0wZDNDytLWL+pnMEDdeoTpVTHUWcSEZEBxpjNxpjvReRwohMyCrDaGFO7h7mDWLOujGCo9g2GLpewKadCk4hSqkOJVxN5g+iMvQAvGmPOaPlw2qYt2ypYuKSItFQ3gwamMH9RYa1EEokYBuooLaVUBxMviVRf47VJ94e0d8YY7n90HW/OyUWI1jYMYEVvWq9q0vJ6LA4c1UlrIUqpDideEjF1PO4w5i7cxVsf5hLca36stFQ3o4ens3h5MT6vxQlH9eanl3bIPKuU6uDiJZFxIlJCtEaS7DzGeW6MMZ1aPLoEe2tOLn5/7f4P2zb85Pwsxo/JaP2glFKqDYm3KFWHX6IvGLLxhANM2PI5I/MW4vek8P2AI8nNmkAoRue6Ukp1NA0Z4tthHTM9gwPv/wWdynfgdaY86V+4lkVlxzJ29PQER6eUUonXkJsNO6zhm74hw7+zKoEAeCNBpm78AIoKExiZUkq1DZpE4ih473PcoUCtcneSl8JvFyUgIqWUals0icSR1KcH4qp9iYxt8HbvkoCIlFKqbdEkEsfAq85HfN6ahSJ4unamy4yJsQ9SSqkORJNIHJ3GjmTsf27DSktjc/8JLBlyFEUHTGPanCcRSy8dQCQQpOCTbyn49FvsYDDR4SilWpmOzqpH0nHH8dRZPdm5049tQCyL+U8Wcecf+uPzduxEsn3OFyw677o9t+9bFpNefoDuR+rINaU6io79LdgAt9+zitx8P5VBCITAH7BZurKEp1/alOjQEiqQX8DCs39GuLSccElZ9HdxKQtOu5rgrqJEh6eUaiWaROKo9EdYuKSQyF73FQaDNu98mJeYoNqIbS++A3aM2XAM5L06p/UDUkolhCaROAIlZZhwOOa2YLBjr2QYKi7FDtTuA7GDIULFJTGOUErtjzSJxLHskdl0L8sFU7MqYtlhJnUra/J5jTGYSPtOQt1nzsCVUnttefG66X70IQmISCmVCJpE4ljxzXpOWvoEvrAfdyT6V7cn7CctUMxUe3Wjz2cHg6z41R3M6TKBd5MP4MuDTqVw7uJmjrp1dJk+gZ4nHIErNbmqzJWaTN+zTqTz+FEJjEwp1Zp0dFYcu3oPZUTlq1z1xe9Y2m86u1J70a9oA0O3LyHttNsbfb7Fl/ya/Lc/xa70A1CyeCXzjruEQ757jbQR7WsqeRFhwnP/JO+ND8l55nVELDIvOZ1ep8xMdGhKqVYkxnSspUImT55sFixY0KB9P/wsjw3nXE7Poo14nPmzQpaH7RkDuWDd66Smees5wx7+rfl8OuLo2v0ILhf9Lz6Nsf/Zk5SMMWx74W02P/ICEX+Afuf/iAFXnIMrydfg11NKqeYiIguNMZNjbdOaSBxHHtqL9y/7IxvefoVRm74BgRX9D+bQO69qVAIBKF+3CSvJVzuJRCKULK3ZNLb0ylvIffk9IuWVAJSuWMu2F95m+uf/w3LrP5lSqu3Qb6Q43C7h73+ZxLenZvHl3J10SnNz1TG9yerf+GVwU4cPwg7UnsxR3G46TxpT9bx05Tq2vfgOduWefe0KP6XL15L/1if0Oe3Ypr0ZpZRqAZpE6uFyCYdM6c4hU7rv03mSeveg79knse2V97Ar/FXlVpKXITdcVvW88KsFiEit4yPlFRR89LUmEaVUm6Kjs1rRgf+5jSG/uhxPtwzE46HrYQcx47PnSRncv2ofb89uiKv2opKWz4uvb8/WDFcppeqlHesNsHlrBS/P3sqWrRWMO6Azp53Yj4zOnhaJLxII8vHAwwjtrLnolSslicOXvU9y/z4t8rpKKVWXeB3rWhOpx/dLC/nJLxYy+/1tLFhSxDOvbObH18xne0Ht/o3m4PJ5mf7R0yQPysSVmowrPRVP1wwmvfygJhClVJujfSJxGGO4477V+AN77lgPBg3hcIhHn93Ab68b2SKvmz5mOEeu/ojSZWuwA0E6jR+lo7KUUm2SfjPFsasoxM7CGPND2TB34a4WfW0RodOBI1r0NZRSal9pc1YcyT4L6ugySk2u3fmtlFIdjSaROFJS3Eyd1BWPu+aQ2ySfxZmnZCYoKqWUajs0idTjxmuH0aO7D0vA5QK3C44+rCenn9Q30aEppVTCtVgSEZH+IvKpiKwQkeUi8gunvKuIfCgia53fXZxyEZH7RSRbRJaKyMRq57rY2X+tiFxcrXySiPzgHHO/xLpLbx+EQja33LaCXYVBbAORCLhcFv37JWNZzfpSSinVLrVkTSQM/NIYMxqYBlwjIqOBm4CPjTHDgI+d5wAnAMOcnyuBhyCadIBbganAFODW3YnH2eeKascd35xv4JOvdrBuU1mN0VmBoM2jz26kqDjUnC+llFLtUoslEWNMrjHme+dxKbAS6AfMAp5ydnsKONV5PAt42kTNBTJEpA9wHPChMWaXMaYQ+BA43tnWyRgz10TvmHy62rmaxeffFuD327XKw2HDjX9aSjhce5tSKvGMMZStWkfpimw62g3Vra1V+kREJAuYAMwDehljcp1NeUAv53E/YEu1w3KcsnjlOTHKY73+lSKyQEQW7Nixo8Fxd053U1cDWfaGcl54PSf2RqVUwhQvXsmnI47mq6ln8PWMM/lk8BEUzluS6LD2Wy2eREQkDXgVuM4YU2PxbacG0eJ/JhhjHjHGTDbGTO7Ro0eDjzvp2D7U9UdMKGx46ZllRCoqmynKhimcu5i5x1zEBz2n8OXkU8mb/VGrvr5SbVm4vIK5x1xE5YYcIhWVRMor8efk8d0JlxLcVZTo8PZLLZpERMRDNIE8Z4x5zSnOd5qicH5vd8q3Av2rHZ7plMUrz4xR3mxytlYSYy7EKoFAhCWX38Kq7FJu/fsKLrt+IQ88to6CnS0zJUrh3MXMO+5idn42j1BhMSVLVrLowl+S8/TrLfJ6SrU3ea99gAmHa5XbkQjbXnwnARHt/1pydJYAjwMrjTH/rLZpNrB7hNXFwJvVyi9yRmlNA4qdZq85wLEi0sXpUD8WmONsKxGRac5rXVTtXM3i4y+3E4nE3mbZYYblLSL3jQ+54Zff8MlXO1idXcar72zlwmsXkJvvj33gPlh1yz+IVNQ8r13hZ+VNf8fY2j+jVCB/B7Y/xiwTFX4CeQ1vylYN15LTnhwMXAj8ICKLnbJbgL8BL4nIZcAm4Gxn27vAiUA2UAFcCmCM2SUifwHmO/v92Rize86RnwJPAsnAe85Ps7HtPW1ZnnAAWywiLg+esJ+kUAWHZc8mEjG4SosxqUlAtNO9vDzMo89u4A+/HNWk1/12wU7eeC+XysowMw/tyQlH98brsShZsjLm/qHiUkJFJXi7ZjTp9ZTaX3SZMQnL5yGyV23ElZZC10NiTkKr9lGLJRFjzFdAXTdTzIyxvwGuqeNcTwBPxChfAIypfUTzGDcyjXkLd4EIh69+Db83lV2pvehfuJYx2+bhjQTwu5Mo9WbUOM420ZFdN/xhKQeO6sSpJ/alS+eGLaf78FPreeWtrVXDilesKeXdj/P419/Gk5TZh7IVa2sdY3k9uNMbv9qiUvubLtMn0PXQg9j1xfyq/korOYnO40fTfeaMBEe3f9I71uMYV7AAMdEv8wWDZnLQpo84ZekTTNzyBd5IgJDl4eMRZxFx1V5bJBCw+W5RYXTq+J/OJ297/c1bO3YGeOnNnBr3pfgDNus3lfPZNwUM/8O1WClJNY5xpSSTdc2PsTwts76JUu2JiDD5tX8z8s5f02nCaNLHjmTkbTcw5f3/IpZ+3bUEvapx5D7xIhM3f4Y7HKAopSdvjr2CiLiwgQiCGJuI5cbE+XAGg4bS0jAPPbm+3tdb9EMRbnftc1X6bb7+bid9zjie0XffgqdrZ6wkXzSBXPtjRvzpun14l0rtXyyPh6yrzufQ717nsIVvMujnF+PyNawlQDWeTgUfR96OAEfnv0jP0hwWZx7KaYv/g8fsbms1YCKcsPxZtnYZQlFKT5KSBL+/9phg28B33xfWKt9bp/TYtQmXBV2clRQHXn4OAy49k+CuYjyd07C8+p9DKZU4WhOJIygeBBi/9Wsumfc3vHbtUR9ibMZsnUuSz+K234zG7Y7dDZTcgKnjJ4/LwOut/U/i9lj86Pg9qxqKy4WvR9cWTSBlq9eT//YnVKzfUv/OSqkOS5NIHH08FQhQ6U5mbfcDCUvtRGCZCCnGz0VnD2Da5O4cOrUbHk/NROLzWg2a9dfttrj/9nH07O4jOclFaoqLpCSLm64dTlb/1uk4D5dXMO+En/DlQaex+OIb+XzcSSw85+fYwdoJVCmltDkrjv4zJ/LenN68M/YSMioKuPzrP4GpeeNIxO1jzCUnctpZAwD4zc9GUPDnH1i7rgyXSwiFDIdN7865p/WP9RK1DB6YyiuPT2VVdil+f4QDRnTC52u9BbBW3HA7u75agO0PYDs3429/7zPW/OVfjPzL9a0Wh1KqfZCONjnZ5MmTzYIFCxq078cPvs8f3/dhxAJjGLJ9CUeufo2uFfl8l3UM8wcfi9+bysiRGVx3xVBGDe9Udez6TeVsy69kyMA0+vRKivMqbYexbd7vNA47ULvW4emWwbF58xIQlVIq0URkoTEm5o02WhOJ46W3tpFsd+e4lc8zbPtiANZ3P4A3xl9JYWqvqqG9y1eV8rNblvDoPycyaEC02WnwwFQGD2xf926YSAQ7VHvKCIBIeevOEaaUah+0TySOXUEPF827k2HbF+MyNi5j069wXY0EslswZPPMS5sTFGnzsDweOo8fXXuDCN2OmNr6ASml2jxNInEMqtxMSrAUl9lz819JSndcdowJ3mxYtaa4NcNrEQf++0+40lIQbzRJWj4v7k5pjL77lgRHppRqizSJxNHN5ccdqbmCYefKAiJWjI5u2yZl8bd8c9i5zTIstrA4yKYtFa2+8FXnSWM4fMk7ZF17Id1nzmDQDT/h8GXvkTZ8UKvG0V74t+bj14n9VIKFS8vY8OAzLDjjGlbc+FfKsze12mtrn0gcdkoqYZcHV2TP1O7JoQpG5S5gVZ/JhF177tNwmxAzst+msHwb3xx2Lkeu+7RJd8mWV4T50z9WMn9xIW6X4HIJP7t8KCcd3btZ3lNDJA/oy+g7f9Nqr9ceFS9eyaILb6By41YwhrRRQ5nw3D812apWF9xZyFdTTidQsAu7wo943Gx+5AUmv/ZQq8wXpjWROAYPzYjO3Fvt/pAIFketfoXJmz4hyWWDMXQv3crZCx+gV2kO2DYVReUseeztJr3mH+5cwfzFhYRChkq/TVl5hHseXsvC7/LIefp11vzlQfLf/gRT1xz1qsWFikqYe/SFlK9aHx0KHQhSsmQl3x55AZEYI9uUaknZf30Yf94ObGeZCBMKE6nws/gnv2mVpYG1JhLH9B8dyAcvdCfDX4DXqY0EXT5SQmWcGPyOy8f2Yu3f/oNl9mpyCgZ48ZEFRA4+koPGd2nw6xXsDLDohyJCoZr/8Mm7ctl09PVslzB2RSWutBRSBvVnxuf/w52ets/vUzXOthfexuw9is0YIhV+8md/RN+zTkxMYKpDynvzI0wwVKs8XFRCxfotpA4Z0KKvrzWROMp3FPHs1BspSO2LZWwsY+ML+ynzdqLHfX8nY9IYPKm17wEJWx62pvTnrn+tadRfAjsLg7g9tf9JTln6BJ7KMmxnautIWQXlazaw5rZ/Nf3NqSar2Lwt5rLIdiCAPyc/ARGpjsydnhKz3ERs3GmxtzUnTSJxvPjyBkbmLaBvyQaE6OIoFoaksJ95T31Gz5OOIHlgP+xqy6bsThnbOg9k+44AZeUNb3YamJmCHamZdHyhCnqXbMbaayl6OxBk2//eauI7U/siafxYTHJyrXLL6yXjoAMTEJHqyLKuuRBXyl6fR5eLzgeNxdere4u/viaROLZUpnDi8mdrDPEF8NhBunz0KpbbTZ+zTsA4SSQ/PZNXJvyURw/5E538RYDB52v4JU5KcnHZBVm4GnqI1LXml2op3y8t5MoXvezwdCdk7blXyEr20XnygXQ5eFICo1MdUf9Lz6TvuSdFh+Onp+JKSyF1WBYTn/tn/Qc3A+0TiWPa2rcQYjdH+Up28vVnm/A//jIubLZ1zuK5g35J2OUBsSgFLAPzFu7i0GkN/2vgvNP68+XcApauKAEg4Ekht/NA+hZtqFEbMR4P/S740T69P9U4oZDNLXesoDIoPD3lRqatf58xud9hLIvePz6NKXdfi2hiV61MLIux/7mdoTdfTfHCZST17UXGtPGt9lnUJBJHIekkJ3cjPz2ToTuW4TY1O1N/OPdassJlAHw84izCbl+N7bYN9z6SzSFTuzXqH/TEmb1Zs66saoXDt8b+hIvm3ok7EsQTCRJ2eUkdOohhv/3pPr5D1RhLVxRX9XGF3El8OfxUvhx+KgCH9unG0brwkUqglKxMUrIyW/11tTkrjm8HHs2nw09nyI5luPZKIAL0LtxAaeYIsCzyO8WepXd7QYCcbY2bd+qYw3vSo7sPj5Pii1J68q/D/8b7B1zIN6NOY/mZv+LoRa/hTmtfc3M1RKSiktJlawjurH8Rr9Zm23UPkohE9n0oZf4OP599vYMfVha3ytBMpZqD1kTiCLl8nLzsqWqrGdYUERf5I6cxPHctKaEyit21R2oZAz++ZgGzTujDNZcOwRdj0am9+XwuHr17Ii++kcPHX27H74+A+AgOPZJJx/Th1BP6YDW446R9MMaw7u+PkH3HQ+CyMKEQvU89hrGP/hVXkq/+E7SCsQdkEOu7PSnJ4oSZTb8Z1BjDPf/J5u0PcnG7LYyB7l293OesLaNUW6ZJJI4JOV8ipu7RVW47TK9R/Tns3nfYeNscnt8YIiS1l7iNRAyvvb2NDz/L5993Tqia6Xe3bxfs5OmXNrO9IMD4Azrzk/Oz6NcnmcsuyOKyC7Ka+23ts215lfgDNgMzU3C5mqfdddsLb5N9x0M1hs7mvfkRruQkxj5yR7O8xr7yeS3+eOMofv+3FRhjCIUMviSLaRO7cvj0po+CmfPpdt79KI9gyBAMRT9v2/Iq+e1fl/Po3RObK3ylWoSuJxLHQz1OJLNoXVWbn4Gqwbwhy8PqXhNIj5Qz3LeL5IH9mFM6iE97HxVdf4TYX64D+6fw3L8Pqnr+xvvbePCxdVX9H5ZEl9J94t5J9OtTexhpIm3Lq+SWO5azZWsllhWtMf3+hpFMndh1n8/9+fiTKVu+tla55fNy7Pbvag9hTKCCXQE+/mIHpWUhDprQlbGjOzWozysYsvlybgHb8vwMH5LGQeO7YFnC5TcsZNXaslr7ez0Wz//nIHr1aB/r0aj9l64n0kRplbuIWB4sO8SWjCGU+TqTtXMVQXcSCwYcyfyso8mo2MH3pTmMyF7EQfmvUhyw+H7AkRir5peKJxxgbM5XbK8Yzra8MfTtnUwoZPPQf9dXJRAA20ClP8ITz2/k9zeMau23XCfbNvz8t0vYXhDAdsKt9Nv89o7lPPXA5H1OeMHtO2NvECFUXNqmkkj3rj7OObVxHZi5+X6u+vUiKisj+AMRfF4X/fsm8+DfxlNREbu263JBRWXjprf5/JsdPPXSZgp2BhgzsjNXXJhVq+arVHPSJBLH14NPYvjOHxhcsJz3D/gxBen9au1TmNqLwrTerOsxhsWZMzhs7WyWZB5C2KrZlm1EmLHuHXxrX6dk3iD6zjqY3O3+mJ21tg2Ll7WtaeUX/VBESWm4KoHsFo4YZs/J5epLBu/T+btMn0j+Wx+zd6eDu1Naq9ww1dJuv2cVhUXBagk4woYt5Tz+3EYOn9GDF97YUmu6G5/PxYB+Db/j+OXZOfzn6Q1Vf5R8Oa+A+YsLeeyfExnYv+XvXFYd0/7VO9vMVvabyuvjruS9Ay6iIK1v7J2cZoyQy8fB696lb/EmJm3+BHckiGWHcUVCuCNBjlv+LGmhMjyRIAV/jd4ElNHZQ7iOUT1trUN1Z2Ew5h0z4bBhSx2jz8Jhm0ef3cCJ53/Nkad/wS9+t4T1m8pj7jvithui65hUGzDgSknigHt+h1jt+2NaURnhh1UltRJwKGT44LN8zj+9Pz26+Uhybkx1uSDJZ/Hb60Y0uM8pGLJ59NmNNWq1xkAgGOHx5zc211tRqhaticRhW26MCMv7Tat3335F6+hTsgkLm6PWvM6B2+axtuc43HaIEXnf09m/q2rfkqWrAOiU5uGwad35ct5OgsE9//mTfBYXnd2yk6Y11gEjO9U5jHXJ8iLKysOkpdb8OP3ln6v4at5OAs57W7ikiKtuXMQz/5pcq50/fdQQDp33Gmvv+DeF3y4iZVAmQ2++mm6HTWmZN9Sa4vQ7GgPpaW6evH8y73+Sx3eLCund08fpJ/ZjQGbDaw/5O/wxhwXbNixfVdKksJVqCE0icaRVFFCe3BW7ruG0xmDZIWyXl77FG7DsPe3XPcq20aNsW8zDvN0yqh7f/PMRcP9qvphbgNslWJZw9cWDmD65W73xGWMo/HohlZu30XnSGFKHD2LXl/Ojo5pSkuh3/izSRw1p1Hsu2BXgldlb+WFVMQP7p3LurEwGZKbgsoTUZFeNZLdbZWWEN9/P5YIz9twrk7fdz5dzdxIM1dw/GLJ5efZWrr2sdlypw7IY/9+/Y2ybovk/YEIh7GAQy9u+b+JLSXEzalg6y1eX1MgnHrdw9GE9ovskuzj9pH6cflLtJtOG6NLZW2eS792jbdVq1f5Fk0gcY7fNJT+9P9m9J9Sep8oYvKEKDtz2LSv6TqPSnUrEcuOORGqM4tqbJCexdPQJ3HnWl4wsX8vMngVcfuQQrvv30ZRGPPTpmYQnxky+ewvkFzD3mIuo3JILAiYcwdu9K8GdhdiVAcTlYsN9TzL67lsYeMW5DXq/W3MrufyG7/EHIoRChpz5a+HuvzLWk8vacDfImgUpvWodFwwZvpm/s0YS2ZRTgccj7D1DdThsWLm27r+Mi79fzvxTryJcWo44gxPGP3UXvU4+qkHvoa363fUjuerGRQSCNpX+CMnJLnp193H5j5tnEau0VDczD+3JJ1/tqKr5Afh8FhedM7BZXkOpWDSJxJEaKGbbwCNjbxTBdrk5LPstjlr9alXSsIGAO4XkcEWN3Q1gY7Gg3xF85prKeZ/fSe+SzbgiQRa/4sOb/Demf/wMnn4jCQQiPPH8Jt75KI9w2OaQqd25+pLBdOuy5y/yxRffSPnaTZjwnhsh/Vty97xeOIwJh1n+i9vY9O/nKM/eRFK/Xgz7/c/IrGPOrYeeXE9ZeRhjoEdpDhfNvROXHSJibLLYyI+KCnh6+k3OEOaa0lJrLhncv29yrY5iALcLhg6KvQZKpNLP3OMuIVxUM8l8f/71HL70nYRM6dBcMvsm8/LjU/n0qx1sy69k+OB0ZkzphruZ7rMBuPHa4bhcwgefbUckmkCu/cmQZhmCrVRdNInEkd1jLH53cp2z5YYtD96wv2piRBvBiIsFA49k+vo5CDYuYxO0vIRcXp6edhNFqT3xhitx2eGqha4I+AkH/Hx/3nUcvuw9brh1KctXlRB2Wsc++CyfhUsK+d/DU0hOchEsLGbnZ/MatLqhCYUoXbYGgIp1m1n20z8QKStn4P+dV2vfBUsKq5pbZq56GU8kUJUcLQx9SjbSpXw7u9Jq3529eFkxxSUhOneK3mzZt3cyk8dnsGBJIcHgnmTi8VicPSt2Mtj+7mcx35OJRMh55g2G//7aet9vW5ac5OLEFlzm2OuxuOnnI/jFlUMpKQ3Rvauv2W4GVe2TbRtmz9nGK29to6IywmHTu3HxOQPp0rn5mojb97CXFpaXkYXtqn0HutgRMIa+RetrzKxrYTAGpm34AMuEwRhshFW9JvLw4bdTlNoTgKA7mZcm/ZywVTOHV2zeyvu3PMXS5cVVCQSinaNFJSE+/Dy64NHqW+9t9PK4IcvLxq4j2OLuyapb78XsPVQISE3ZE0+/onW1muQEOHfBvbD3So5Em6ne+iC3RtlffjOak4/ug89rIQIjh6Zx/+3j6Nc79j0fwZ1FmHCMJBIM1X0fiaolOclFrx5JmkAUf3tgNQ8+vp6NWyrYXhDgjfdyuey67ymviD2VU1NoTSSO3iWb2eRJJbJXInHbYUZs+46AN5UPRp5D3+INjMz/HrcdxkUkmmSA3UtUjcpfyEeja/ZL2GKR3WMsI/O/ryoz/iCzP9qB6ZVVq1MlFDLM+TSfww/0sfmRFxr1Ppb2ncYHoy9AjI0RISVUzpAVOxg6pmb/xpmn9OOx5zYSCNhUelLxRmqvF54SLEWg1nDfQNBm+eqazVA+n4sbrh7G9VcNxbap90ut2xFTY45kEo+booXLWH/vf+l/6Zl4Oqc36H2rPfz+CK+/t42PvthOcpKL007sy1GH9NCp6/djufl+Pvp8R43BLeGwoaQ0xDsf5tXZItBYWhOJY/SWb7D3av+37DAGWNlvCtm9xrMwaybvjLmY/xzyZ7an9ol5HltcDCpYUaMs7PJQ7q39ZZiX2q/O5rNlK0s4+9ql7Eiuo0nEshCvByslGVdaCoiQn57JnAMuIOT2EfQkE3InUZzUhV/dtb7GaJ6teZWsyS6ryg7zBx1L0KpZ5bU9XlYOPixmn4jHIwzJ2nNndDhiKC4JYdsGEcHlEgJBm/Wbyikuqb0eNEDa8EH0v/h0XKk1ayomHKZ4/lJW33ovnx94Iv68HbHfv4opFLL56W8W89izG1mdXcbiZcXccd8q7n6o9jQzav+xKrsUt7v2d4k/YPP9D0XN9jqaROLI75LF+M2f06MkBzE2lh0hKVhGxHIRcfmqvkyNWPjdyWzNGFzHElbUqlnY4uKz4adSUG20k4G49xREbCgPCq+Nvyrm62RMGceEp++m53GH0u/8HzH20TtYPOgoIrJXhVMsKiojLFleRDhsc/9j2Zx75Xd8+MX2qpE98wccyaL+hxG2PEhaKpbPS+aZx/HrxfcyekQ6nr0+nB63xanH98W2DY89t4ETzv2aUy/5lpN//A1vvr+NF97YwskXfM1VNy7i1Eu+5fd/Wx6dnXgvBzxwK+OfvIvuxx6CtXv2XufN2hV+gjt2sfZP99d5jVRtn369g81by2uM2goEDG++n8umnNg3f6r2r0c3X8x7h9xuqbNJuSl0AsY4zp3xCGkVBXgEBhasIOhJYU3P8YzMX8gnI85kzLZ5jMqbT8iVxOYuQ0kKlDNx6+fkdBnGsr7TsUUYs20eA3eu4oEj7yLocf7hjAERLDuMJ1zJkWveYEvXYXSq3ElqoJjPh59ByF332H53JMgVX91KRmXNfgJPj66EdhVFsw2ACLMP/xXLk4bVOkdykovhQ1JZsbqUULjuz4AvVEGm2clDTx5PSp/o9CNl5WHufmgtn369A9s2DO3v4xzm0XnVQha4h5Iy/wu6luVTnNyVL4bNYn3mZBBqjNbyei0On9aNS7suY/MjLxIJBOh04AjCZeW4UlPoPetollz6G+xA7SY1b/cuHJM7t+p5qLiUNX+6n63PzSbi95Pcvw+DrruUzAtPw6ULRXHbP1fx/qf5MbdNn9yVu27VdeH3R8YYLrp2AZtzKqq+EiB6M3Nj57uLNwFju08iInI8cB/gAh4zxvwt3v6NSSKnHvwYJ616me7luXgjAWyEiOXh86EnM33DB3giQbx29EsuZHlw2yH+N/l6cjMGVyUBT9hPUrCcATtXsjxzBuzdFGRsXHaEiMuDKxLGMhEyC9ewuevIWn0xu7kjQS7/6o90qSzYc5rd12OvfZf2nc4HB5xPyNX0G84sCx7863jGju5cozwcMRQuXMHC436MHQgSCYWJWB489p7mqqDl5V9H/BV/jKY7NxGu//pm3KVFtV8zJQnbH4jOSBkjoJmbviCpdw/scJgvJ51K+ZoNNYY7YwnpB47k4C9fwJXcsWfBffip9Tz7ypaY25J8Fh+9cmgrR6Ray87CIH+8awXLVpZgWULnTh5+e90IJo3r0qjzxEsi7bo5S0RcwL+AE4DRwHkiMrq5zj+yYDndy7ZVDcW1MHjsIEesfZPkUHlVAgGqvjhLUrrVqEWE3EmUJnelZ9k2vGF/jDdhVSWLiMtNyO1je/oALvq27jU00vxFZFRLIBBNHrF6UkbnzqdrWR4es+eLPXpnfH3vfo/oHEy1R2S5XcKaG/5MpKwCEwoDUiOBAHjtIC479kgyiYQpC7tibrMrYlyrqgNh/T1PANFhwZWbt9ZMIAC2oWxlNpsaOQhhf3TKcbH76iD679re/5BUdevWxcsDd4zn9Sen8+y/D+LVJ6Y2OoHUp10nEWAKkG2MWW+MCQIvALOa6+SDdq6skSh2E2NqDO3dLSIuMguzY56rIK1v7VpIHYJuHxGX0wxjTNWPJxzAF6rgtCWP1HlH/N7cJsxF8+5kZvZrTBjTmUOndWNAZnKtyQDjEYEDR3aqVR6dnmTJnv3q6BEasGtNzHKXHaaTv+5lcMXjiT3IIGJT8NHXABQvWkGkrKL2PkSHBue++E6d5+8o+vVOZmBm7KaLUcPSdYRWB5DR2UOfXkkt8m/d3of49gOq19NzgKl77yQiVwJXAgwY0PCJDcOWN+YUJgZilgtQ6Y69dkPXiu0kB8sIurz1JhMjFqt6TUCMTXrlTmase4/S5C6k+4sYlbeApHDj1mx322GmB5Yz86/jAbjzgdVs2lKznTSen146mKSkGDUGEVxJPiJOraGuj+e0jR+wYdA0giG7Knn53IZj17yBFeOek90stwvbtp1aTk3J/aOzKqcO6o+VnIRdGbvm4krXtTQA/vDLUVx782ICwei/gcuK9kvdcFXt/jKlGqO910QaxBjziDFmsjFmco8ePRp83Koe4whbNfslDGBbsZtgxNhs6B67NW1E/kLOWXgfnfyFeMOVeEOViB3Gsmt+QYodISVQzIKBR2OAC767m3Fbv+aw7LeYkPNlVQIJiQu7gfURSU5i8A0/qXp+zqmZdc7P1TXDTXqaG7dL6NPLx99+dwDnnto/5r4iQuYlZ+4ZRRVD2O1jxOWn8tQDkznuyF70653ExLEZ/PWW0Uwo/aHO4cwAWBZdpk9AvDX/DVwpSQz+5WUA9DnzeNypsf/KtlKSyLr6grrP34GMGJrOk/dP5pRj+zB6eDonH9uH/943mZHD9J4btW/adce6iEwH/miMOc55fjOAMeavdR3TmI71QCDAX0f9jIk5X1XdL2KZCGJsIrhwU7Otf1G/Q5gz5sJa5zlo/fuM3/o1XSui9zdsyxhMpSeF3kUb+HzEGazoc1DVX+S2WNgi9K7MZ1dGJknFBZy+6CG6lecB4HZbdJ0xgY8LehAuKWdizhfYWAjgtoO1a0deD1nXXMioO39doyo7f3Ehf39wDQW7AmCio3RuuHoY3bs2rgM+UhmdrqXg42+wfF7C5RVYLhcmHMGVnsrQm69i8PU/iVmNLl25joVnXUvl5m2YSAQTCuFKTgaXheV2M/m1f5N+wDAW/fgGdn7+HeLxIC6L0XffQv+LT686T3n2Jr4/7xeULF4ZLbAsxO1m4FXnMfofN2tzjVL7aL8dnSUibmANMBPYCswHzjfGLK/rmMYkkd0uGvdXegYK6VqRD5Egtm2T23cMQ088iPT3XsddUYZ9xnn062qRUllETp/RrF5TQqarmAGplQQ69aJXuICktT9gedxEysqxknz0PeskAv2z+OQfsykIekgeOoj0YCkHDkvhgDMORtxuwmHD9oIAecs20b+70GPcMMSyMMaQ/cFC1ny0hO7BHQw4YgJdZ0xi28vvUbpiLZ3Hj6bTuJGkjRxS5x3exhgKi0IkJblISY5du2qo8nWbKV+7gbQRg0nOyiRSUYkrOaneBaWMMVSs24wdDJGU2YtdXy3E5fPS9bCDsDx7aiCB/AKCBYWkDhtY59TwgV1FFH45n0iFny4zJpIysGnTqiulatpvkwiAiJwI3Et0iO8Txpjb4+3flCSilFIdWbwk0t471jHGvAu8m+g4lFKqI+oQHetKKaVahiYRpZRSTaZJRCmlVJNpElFKKdVk7X50VmOJyA5gUxMP7w4U1LtXx6XXp356jeqn1yi+RFyfgcaYmHdqd7gksi9EZEFdw9yUXp+G0GtUP71G8bW166PNWUoppZpMk4hSSqkm0yTSOI8kOoA2Tq9P/fQa1U+vUXxt6vpon4hSSqkm05qIUkqpJtMkopRSqsk0iTSAiBwvIqtFJFtEbkp0PC1JRPqLyKciskJElovIL5zyriLyoYisdX53ccpFRO53rs1SEZlY7VwXO/uvFZGLq5VPEpEfnGPul3a64IeIuERkkYi87TwfJCLznPf1ooh4nXKf8zzb2Z5V7Rw3O+WrReS4auXt/jMnIhki8oqIrBKRlSIyXT9He4jI9c7/sWUi8ryIJLXLz5AxRn/i/BCdYn4dMBjwAkuA0YmOqwXfbx9govM4neh6LaOBvwM3OeU3AXc6j08E3iO6Ou40YJ5T3hVY7/zu4jzu4mz7ztlXnGNPSPT7buK1ugH4H/C28/wl4Fzn8cPA1c7jnwIPO4/PBV50Ho92Pk8+YJDzOXPtL5854CngcuexF8jQz1HVtekHbACSq312LmmPnyGtidRvCpBtjFlvjAkCLwCzEhxTizHG5BpjvncelwIriX7gZxH9UsD5farzeBbwtImaC2SISB/gOOBDY8wuY0wh8CFwvLOtkzFmron+L3i62rnaDRHJBE4CHnOeC3AU8Iqzy97XaPe1ewWY6ew/C3jBGBMwxmwAsol+3tr9Z05EOgOHAY8DGGOCxpgi9HNUnRtIdhbXSwFyaYefIU0i9esHbKn2PMcp2+85VeYJwDyglzEm19mUB/RyHtd1feKV58Qob2/uBX4N2M7zbkCRMSbsPK/+vqquhbO92Nm/sdeuPRkE7AD+6zT5PSYiqejnCABjzFbgH8BmosmjGFhIO/wMaRJRMYlIGvAqcJ0xpqT6Nucvvw47NlxETga2G2MWJjqWNswNTAQeMsZMAMqJNl9V6cifI6cvaBbRZNsXSAWOT2hQTaRJpH5bgf7Vnmc6ZfstEfEQTSDPGWNec4rznSYEnN/bnfK6rk+88swY5e3JwcCPRGQj0WaCo4D7iDbB7F4ttPr7qroWzvbOwE4af+3akxwgxxgzz3n+CtGkop+jqKOBDcaYHcaYEPAa0c9Vu/sMaRKp33xgmDNqwku0U2t2gmNqMU476+PASmPMP6ttmg3sHhlzMfBmtfKLnNE104Bip7liDnCsiHRx/uo6FpjjbCsRkWnOa11U7VztgjHmZmNMpjEmi+jn4RNjzAXAp8CZzm57X6Pd1+5MZ3/jlJ/rjLwZBAwj2lnc7j9zxpg8YIuIjHCKZgIr0M/RbpuBaSKS4sS/+/q0v89QokcptIcfoiNH1hAd7fDbRMfTwu/1EKJNDEuBxc7PiUTbXz8G1gIfAV2d/QX4l3NtfgAmVzvXT4h29GUDl1Yrnwwsc455EGfmhPb4AxzBntFZg4n+B84GXgZ8TnmS8zzb2T642vG/da7DaqqNLtofPnPAeGCB81l6g+joKv0c7Yn/T8Aq5z08Q3SEVbv7DOm0J0oppZpMm7OUUko1mSYRpZRSTaZJRCmlVJNpElFKKdVkmkSUUko1mSYRpRpBRCIisrjaT1YTznGqiIxugfB2n/99ESkSZ3ZhpVqSu/5dlFLVVBpjxu/jOU4F3iZ6c1mDiIjb7JlTqT53EZ3Q7/8aH5pSjaM1EaX2kbOuxecislBE5lSb1uMKEZkvIktE5FXn7uQZwI+Au5yazBAR+UxEJjvHdHemU0FELhGR2SLyCfCxiKSKyBMi8p0zqWHMWVmNMR8Dpa3y5lWHp0lEqcZJrtaU9bozz9gDwJnGmEnAE8Dtzr6vGWMOMsaMIzql/mXGmG+ITj9xozFmvDFmXT2vN9E59+FE70z+xBgzBTiSaCJKbYH3qFSDaXOWUo1TozlLRMYAY4APo1Mg4SI6tTfAGBG5jehiTGlE54FqrA+NMbucx8cSnfjxV87zJGAA0QSlVEJoElFq3wiw3BgzPca2J4FTjTFLROQSovNsxRJmT6tA0l7byvd6rTOMMaubHK1SzUybs5TaN6uBHiIyHaLT6IvIAc62dCDXafK6oNoxpc623TYCk5zHZ1K3OcDPnFlfEZEJ+x6+UvtGk4hS+8BElx49E7hTRJYQnfV4hrP590RXhfya6Gytu70A3Oh0jg8husLd1SKyCOge5+X+AniApSKy3Hlei4h8SXTG15kikiMixzX1/SlVH53FVymlVJNpTUQppVSTaRJRSinVZJpElFJKNZkmEaWUUk2mSUQppVSTaRJRSinVZJpElFJKNdn/A6wQiDR1jZixAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_imdb_dataset(num_samples=100, max_length=500):\n",
    "    # Load the dataset\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Since we're using padding, we'll treat each index as a categorical feature without scaling\n",
    "    # StandardScaler is not applicable here; you might consider different preprocessing if needed\n",
    "    \n",
    "    # Print the last 10 values of y\n",
    "    print(\"Last 10 values of y: \", y_train[-10:])\n",
    "    \n",
    "    \n",
    "    # print first 10 values of X\n",
    "    print(\"First 10 values of X: \", x_train[:10])\n",
    "    # cut x and y into num samples\n",
    "    x_train = x_train[:num_samples]\n",
    "    y_train = y_train[:num_samples]\n",
    "    # create np array of both \n",
    "    x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "    print(\"Shape of x_train: \", x_train.shape)\n",
    "    y_train = np.array(y_train)\n",
    "    return x_train, y_train\n",
    "\n",
    "# Usage\n",
    "X, y = load_imdb_dataset(1000,2)\n",
    "\n",
    "# plot the features of X to y value (0/1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('IMDB Data')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM model on the test set: 0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tfidf_transform(x_train, num_features=10000):\n",
    "    # Initialize tokenizer with a given number of words\n",
    "    tokenizer = Tokenizer(num_words=num_features)\n",
    "    tokenizer.fit_on_sequences(x_train)\n",
    "    \n",
    "    # Convert sequences to text list\n",
    "    x_train_text = [\" \".join(map(str, sequence)) for sequence in x_train]\n",
    "    \n",
    "    # Apply TF-IDF transformation\n",
    "    tfidf = TfidfVectorizer(max_features=num_features, dtype=np.float32)\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_text)\n",
    "    \n",
    "    return x_train_tfidf.toarray()\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def select_features(X, y, k=500):\n",
    "    # Apply Chi-squared test to select the top k features\n",
    "    chi2_selector = SelectKBest(chi2, k=k)\n",
    "    X_kbest_features = chi2_selector.fit_transform(X, y)\n",
    "    return X_kbest_features\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def apply_lda(X, y):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    X_lda = lda.fit_transform(X, y)\n",
    "    return X_lda\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(X, y):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(X[:, 0], np.zeros_like(X[:, 0]), c=y, cmap='coolwarm', alpha=0.5)\n",
    "    plt.colorbar()  # Show color scale\n",
    "    plt.xlabel('Projected Feature')\n",
    "    plt.ylabel('Dummy Zero Axis')\n",
    "    plt.title('IMDB Data Projection via LDA')\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def load_and_preprocess_imdb(num_samples=1000, max_length=500, num_features=10000, top_k_features=500):\n",
    "    # Load the dataset\n",
    "    (x_train, y_train), _ = imdb.load_data(num_words=num_features)\n",
    "    x_train = x_train[:num_samples]\n",
    "    y_train = y_train[:num_samples]\n",
    "    \n",
    "    # Pad sequences\n",
    "    x_train_padded = pad_sequences(x_train, maxlen=max_length)\n",
    "    \n",
    "    # Convert sequences to a list of strings\n",
    "    tokenizer = Tokenizer(num_words=num_features)\n",
    "    tokenizer.fit_on_sequences(x_train_padded)\n",
    "    x_train_text = [\" \".join(map(str, seq)) for seq in x_train_padded]\n",
    "\n",
    "    # Apply TF-IDF transformation\n",
    "    tfidf = TfidfVectorizer(max_features=num_features, dtype=np.float32)\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_text)\n",
    "\n",
    "    # Convert TF-IDF sparse output to a dense array\n",
    "    x_train_tfidf_dense = x_train_tfidf.toarray()\n",
    "\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(chi2, k=top_k_features)\n",
    "    x_train_kbest = selector.fit_transform(x_train_tfidf_dense, y_train)\n",
    "\n",
    "    # Apply LDA\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    x_train_lda = lda.fit_transform(x_train_kbest, y_train)\n",
    "    \n",
    "    return x_train_lda, y_train\n",
    "# Load and preprocess the data\n",
    "X_processed, y_processed = load_and_preprocess_imdb()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the SVM model on the test set: {accuracy:.2f}\")\n",
    "\n",
    "# Optional: Visualization of decision boundary\n",
    "plt.scatter(X_processed[:, 0], np.zeros_like(X_processed[:, 0]), c=y_processed, cmap='coolwarm', alpha=0.5)\n",
    "plt.scatter(X_test[:, 0], np.zeros_like(X_test[:, 0]), c=y_pred, marker='x')\n",
    "plt.colorbar()  # Show color scale\n",
    "plt.xlabel('Projected Feature by LDA')\n",
    "plt.ylabel('Dummy Zero Axis')\n",
    "plt.title('IMDB Data Projection and SVM Decision Boundary')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 values of y:  [ 1  1  1  1  1 -1 -1  1 -1 -1]\n",
      "X shape:  (200000, 8)\n",
      "y shape:  (200000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.32919639, -0.35070302, -0.67340164, ..., -0.62987411,\n",
       "         -0.69378638, -0.58988026],\n",
       "        [-0.51370827, -0.99752431, -0.71423781, ..., -0.8880327 ,\n",
       "         -0.226683  , -0.62179136],\n",
       "        [-0.77826341, -0.81688626, -0.96313581, ..., -0.94694026,\n",
       "         -0.38384426, -0.69624167],\n",
       "        ...,\n",
       "        [ 0.19967671,  0.91949855,  0.69596593, ...,  0.91069955,\n",
       "          0.88355407,  1.1106231 ],\n",
       "        [-1.24416957, -1.05789773, -1.06342509, ..., -1.14555687,\n",
       "         -1.51647387, -1.04356356],\n",
       "        [-0.91826969, -1.09715139, -1.13913872, ..., -1.36532045,\n",
       "         -1.42941929, -1.27194863]]),\n",
       " array([-1, -1, -1, ...,  1, -1, -1], dtype=int64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # print last 10 values of y\n",
    "    print(\"Last 10 values of y: \", y[-10:])\n",
    "    \n",
    "    # print the shape of X and y\n",
    "    print(\"X shape: \", X.shape)\n",
    "    print(\"y shape: \", y.shape)\n",
    "    return X, y\n",
    "\n",
    "load_dataset(\"toydata_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svm_loss(W, X, y, C):\n",
    "    distances = 1 - y * np.dot(X, W)\n",
    "    hinge_loss = C * np.mean(np.maximum(0, distances))\n",
    "    regularization_loss = 0.5 * np.dot(W, W)\n",
    "    return regularization_loss + hinge_loss\n",
    "\n",
    "def svm_gradient(W, X_batch, y_batch, C):\n",
    "    distances = 1 - y_batch * np.dot(X_batch, W)\n",
    "    dw = np.zeros_like(W)\n",
    "    for ind, d in enumerate(distances):\n",
    "        if d > 0:\n",
    "            # Only the misclassified points affect the gradient\n",
    "            dw += C * (-y_batch[ind] * X_batch[ind])\n",
    "    dw /= len(y_batch)  # Average over the batch\n",
    "    dw += W  # Add gradient of the regularization term\n",
    "    return dw\n",
    "\n",
    "def train_linear_svm(X, y, batch_size=10, C=1.0, epochs=1000, lr=0.01):\n",
    "    w = np.random.randn(X.shape[1]) / np.sqrt(X.shape[1])  # Improved initialization\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(len(y))  # Shuffle the data\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(start + batch_size, len(y))\n",
    "            X_batch = X[perm[start:end]]\n",
    "            y_batch = y[perm[start:end]]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            w -= lr * grad\n",
    "        # Optionally, decrease learning rate gradually\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            lr /= 2\n",
    "    return w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_adagrad(X, y, batch_size=10, C=1.0, epochs=1000, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a linear SVM using AdaGrad optimization.\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    - batch_size: size of the mini-batch\n",
    "    - C: regularization strength\n",
    "    - epochs: number of training epochs\n",
    "    - lr: initial learning rate\n",
    "    \"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    epsilon = 1e-8  # smoothing term to avoid division by zero\n",
    "    gradient_accumulate = np.zeros(X.shape[1])\n",
    "    n_batches = int(len(y) / batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            gradient_accumulate += grad ** 2\n",
    "            adjusted_lr = lr / (epsilon + np.sqrt(gradient_accumulate))\n",
    "            w = w - adjusted_lr * grad\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rff_gaussian_transform(X, omega, b):\n",
    "    \"\"\"\n",
    "    Apply the Random Fourier Features transformation to the dataset.\n",
    "    - X: feature matrix\n",
    "    - omega: frequencies matrix\n",
    "    - b: bias terms\n",
    "    \"\"\"\n",
    "    X_features = np.dot(X, omega) + b\n",
    "    return np.cos(X_features)\n",
    "\n",
    "def train_svm_rff(X, y, gamma, n_features, C=1.0, epochs=1000, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a SVM with RFF approximation for a Gaussian kernel.\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    - gamma: parameter of Gaussian kernel\n",
    "    - n_features: number of random features\n",
    "    - C: regularization strength\n",
    "    - epochs: number of training epochs\n",
    "    - lr: learning rate\n",
    "    \"\"\"\n",
    "    # Create Random Fourier Features transformation parameters\n",
    "    omega = np.random.normal(scale=2*np.pi*gamma, size=(X.shape[1], n_features))\n",
    "    b = np.random.uniform(0, 2*np.pi, size=n_features)\n",
    "\n",
    "    # Transform training data\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    w = train_linear_svm(X_transformed, y, C=C, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return w, omega, b\n",
    "\n",
    "def predict_rff(X, w, omega, b):\n",
    "    \"\"\"\n",
    "    Predict with the SVM trained on RFF-transformed data.\n",
    "    - X: original feature matrix\n",
    "    - w: trained weight vector\n",
    "    - omega: frequencies matrix used for RFF\n",
    "    - b: biases used for RFF\n",
    "    \"\"\"\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    return np.sign(np.dot(X_transformed, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 values of y:  [1 1 0 1 1 1 0 0 1 0]\n",
      "First 10 values of X:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
      " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])\n",
      " list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 71690, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 74170, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 77842, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901])\n",
      " list([1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32])\n",
      " list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 15455, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 25242, 24762, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 16126, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 40732, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110])\n",
      " list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 15201, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 13209, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])]\n",
      "Shape of x_train:  (500, 1)\n",
      "[0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      "Accuracy of Linear SVM: 0.5\n",
      "Last 10 values of y:  [1 1 0 1 1 1 0 0 1 0]\n",
      "First 10 values of X:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
      " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])\n",
      " list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 71690, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 74170, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 77842, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901])\n",
      " list([1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32])\n",
      " list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 15455, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 25242, 24762, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 16126, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 40732, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110])\n",
      " list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 15201, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 13209, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])]\n",
      "Shape of x_train:  (500, 2)\n",
      "[0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      "Accuracy of Linear SVM: 0.0\n",
      "Last 10 values of y:  [1 1 0 1 1 1 0 0 1 0]\n",
      "First 10 values of X:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
      " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])\n",
      " list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 71690, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 74170, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 77842, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901])\n",
      " list([1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32])\n",
      " list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 15455, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 25242, 24762, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 16126, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 40732, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110])\n",
      " list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 15201, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 13209, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])]\n",
      "Shape of x_train:  (500, 5)\n",
      "[0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      "Accuracy of Linear SVM: 0.39\n",
      "Last 10 values of y:  [1 1 0 1 1 1 0 0 1 0]\n",
      "First 10 values of X:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
      " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])\n",
      " list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 71690, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 74170, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 77842, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901])\n",
      " list([1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32])\n",
      " list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 15455, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 25242, 24762, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 16126, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 40732, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110])\n",
      " list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 15201, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 13209, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])]\n",
      "Shape of x_train:  (500, 10)\n",
      "[0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      "Accuracy of Linear SVM: 0.17\n",
      "Last 10 values of y:  [1 1 0 1 1 1 0 0 1 0]\n",
      "First 10 values of X:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
      " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
      " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])\n",
      " list([1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 71690, 4183, 17, 369, 37, 215, 1345, 143, 32677, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 26441, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 74170, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 33740, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 77842, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901])\n",
      " list([1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32])\n",
      " list([1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6, 20, 750, 11, 141, 4299, 5, 15455, 4441, 102, 28, 413, 38, 120, 5533, 15, 4, 3974, 7, 5369, 142, 371, 318, 5, 955, 1713, 571, 25242, 24762, 122, 14, 8, 72, 54, 12, 86, 385, 46, 5, 14, 20, 9, 399, 8, 72, 150, 13, 161, 124, 6, 155, 44, 14, 159, 170, 83, 12, 5, 51, 6, 866, 48, 25, 842, 4, 1120, 25, 238, 79, 4, 547, 15, 14, 9, 31, 7, 148, 16126, 102, 44, 35, 480, 3823, 2380, 19, 120, 4, 350, 228, 5, 269, 8, 28, 178, 1314, 2347, 7, 51, 6, 87, 65, 12, 9, 979, 21, 95, 24, 3186, 178, 11, 40732, 14, 9, 24, 15, 20, 4, 84, 376, 4, 65, 14, 127, 141, 6, 52, 292, 7, 4751, 175, 561, 7, 68, 3866, 137, 75, 2541, 68, 182, 5, 235, 175, 333, 19, 98, 50, 9, 38, 76, 724, 4, 6750, 15, 166, 285, 36, 140, 143, 38, 76, 53, 3094, 1301, 4, 6991, 16, 82, 6, 87, 3578, 44, 2527, 7612, 5, 800, 4, 3033, 11, 35, 1728, 96, 21, 14, 22, 9, 76, 53, 7, 6, 406, 65, 13, 43, 219, 12, 639, 21, 13, 80, 140, 5, 135, 15, 14, 9, 31, 7, 4, 118, 3672, 13, 28, 126, 110])\n",
      " list([1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 1109, 23, 94, 15201, 151, 111, 211, 469, 4, 20, 13, 258, 546, 1104, 7273, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2849, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 1242, 1179, 20, 19, 6, 1009, 7, 1987, 189, 5, 6, 8419, 7, 2723, 13209, 95, 1719, 6, 6035, 7, 3912, 7144, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 1041, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 1167, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50])]\n",
      "Shape of x_train:  (500, 20)\n",
      "[0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
      " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      "Accuracy of Linear SVM: 0.24\n"
     ]
    }
   ],
   "source": [
    "#  here i try to get some usefull stuff from the imdb data lol\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict using the trained weights of the SVM.\n",
    "    - X: feature matrix\n",
    "    - w: weight vector\n",
    "    \"\"\"\n",
    "    return np.sign(np.dot(X, w))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions.\n",
    "    - y_true: true labels\n",
    "    - y_pred: predicted labels\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load your dataset here (e.g., IMDB or your CSV dataset)\n",
    "    data_parameter = [1,2,5,10,20]\n",
    "    for i in data_parameter:\n",
    "        X, y = load_imdb_dataset(500,i)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        print(y_train)\n",
    "        w_linear = train_linear_svm(X_train, y_train, batch_size=10, C=0, epochs=1, lr=0.01)\n",
    "        predictions_linear = predict(X_test, w_linear)\n",
    "        acc_linear = accuracy(y_test, predictions_linear)\n",
    "        print(\"Accuracy of Linear SVM:\", acc_linear)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with C=0.1, lr=1e-05, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.01, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.01, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.01, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.1, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.1, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.1, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.5, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.5, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.5, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1.0, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1.0, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1.0, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=2, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=2, batch_size=20\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=2, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=1e-05, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=1e-05, batch_size=20\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=1e-05, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.0001, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.0001, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.0001, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.01, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.01, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.01, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.1, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.1, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.1, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.5, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.5, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=0.5, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=1.0, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=1.0, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=1.0, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=2, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=2, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=1.0, lr=2, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=1e-05, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=1e-05, batch_size=20\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=1e-05, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.0001, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.0001, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.0001, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.01, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.01, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.01, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.1, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.1, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.1, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.5, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.5, batch_size=20\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=0.5, batch_size=50\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=1.0, batch_size=10\n",
      "[ 1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.\n",
      "  1. -1.]\n",
      "Accuracy of Linear SVM: 0.97\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=1.0, batch_size=20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy of Linear SVM: 0.0\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=1.0, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=2, batch_size=10\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=2, batch_size=20\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=10.0, lr=2, batch_size=50\n",
      "[-1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1.]\n",
      "Accuracy of Linear SVM: 0.03\n",
      "Accuracy of AdaGrad SVM: 0.97\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.001, n_features=100\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.001, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.001, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.01, n_features=100\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.01, n_features=500\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.01, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.1, n_features=100\n",
      "Accuracy of RFF SVM: 0.51\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.1, n_features=500\n",
      "Accuracy of RFF SVM: 0.39\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=10, gamma=0.1, n_features=1000\n",
      "Accuracy of RFF SVM: 0.94\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.001, n_features=100\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.001, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.001, n_features=1000\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.01, n_features=100\n",
      "Accuracy of RFF SVM: 0.975\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.01, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.01, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.1, n_features=100\n",
      "Accuracy of RFF SVM: 0.03\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.1, n_features=500\n",
      "Accuracy of RFF SVM: 0.7\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=20, gamma=0.1, n_features=1000\n",
      "Accuracy of RFF SVM: 0.08\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.001, n_features=100\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.001, n_features=500\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.001, n_features=1000\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.01, n_features=100\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.01, n_features=500\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.01, n_features=1000\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.1, n_features=100\n",
      "Accuracy of RFF SVM: 0.59\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.1, n_features=500\n",
      "Accuracy of RFF SVM: 0.32\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=1e-05, batch_size=50, gamma=0.1, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.001, n_features=100\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.001, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.001, n_features=1000\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.01, n_features=100\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.01, n_features=500\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.01, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.1, n_features=100\n",
      "Accuracy of RFF SVM: 0.92\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.1, n_features=500\n",
      "Accuracy of RFF SVM: 0.485\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=10, gamma=0.1, n_features=1000\n",
      "Accuracy of RFF SVM: 0.96\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.001, n_features=100\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.001, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.001, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.01, n_features=100\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.01, n_features=500\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.01, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.1, n_features=100\n",
      "Accuracy of RFF SVM: 0.51\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.1, n_features=500\n",
      "Accuracy of RFF SVM: 0.905\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=20, gamma=0.1, n_features=1000\n",
      "Accuracy of RFF SVM: 0.85\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50, gamma=0.001, n_features=100\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50, gamma=0.001, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50, gamma=0.001, n_features=1000\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50, gamma=0.01, n_features=100\n",
      "Accuracy of RFF SVM: 0.47\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50, gamma=0.01, n_features=500\n",
      "Accuracy of RFF SVM: 0.53\n",
      "\n",
      "\n",
      "Training with C=0.1, lr=0.0001, batch_size=50, gamma=0.01, n_features=1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-27-f7ed59704402>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    131\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Average Accuracy of RFF SVM:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mavg_acc_rff\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"__main__\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 133\u001B[1;33m     \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-27-f7ed59704402>\u001B[0m in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     68\u001B[0m                             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Training with C={C}, lr={lr}, batch_size={batch_size}, gamma={gamma}, n_features={n_features}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m                             \u001B[1;31m# Train SVM using RFF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m                             \u001B[0mw_rff\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0momega\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_svm_rff\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgamma\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgamma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_features\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mC\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mC\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m                             \u001B[0mpredictions_rff\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredict_rff\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw_rff\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0momega\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m                             \u001B[0macc_rff\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictions_rff\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-12-d8af6e28b516>\u001B[0m in \u001B[0;36mtrain_svm_rff\u001B[1;34m(X, y, gamma, n_features, C, epochs, lr)\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;31m# Transform training data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[0mX_transformed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrff_gaussian_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0momega\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m     \u001B[0mw\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_linear_svm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_transformed\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mC\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mC\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0momega\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-23-f4533d3477f6>\u001B[0m in \u001B[0;36mtrain_linear_svm\u001B[1;34m(X, y, batch_size, C, epochs, lr)\u001B[0m\n\u001B[0;32m     28\u001B[0m             \u001B[0mX_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mperm\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mend\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m             \u001B[0my_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mperm\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mend\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m             \u001B[0mgrad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msvm_gradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mC\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m             \u001B[0mw\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[0mlr\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m         \u001B[1;31m# Optionally, decrease learning rate gradually\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-23-f4533d3477f6>\u001B[0m in \u001B[0;36msvm_gradient\u001B[1;34m(W, X_batch, y_batch, C)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0msvm_gradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mC\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m     \u001B[0mdistances\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0my_batch\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mW\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m     \u001B[0mdw\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mind\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdistances\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict using the trained weights of the SVM.\n",
    "    - X: feature matrix\n",
    "    - w: weight vector\n",
    "    \"\"\"\n",
    "    return np.sign(np.dot(X, w))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions.\n",
    "    - y_true: true labels\n",
    "    - y_pred: predicted labels\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load your dataset here (e.g., IMDB or your CSV dataset)\n",
    "    #X, y = load_imdb_dataset(500,10)  # or load_dataset(\"path/to/dataset.csv\")\n",
    "    #X, y = load_dataset(\"toydata_large.csv\")\n",
    "    X, y = load_and_preprocess_imdb()\n",
    "    # in y convert 0 to -1\n",
    "    y = np.where(y == 0, -1, 1)\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Parameters to explore\n",
    "    C_values = [0.1, 1.0, 10.0]\n",
    "\n",
    "    lr_values = [0.00001,0.0001,0.01, 0.1, 0.5,1.0,2]\n",
    "    batch_size_values = [10, 20, 50]\n",
    "    gamma_values = [0.001, 0.01, 0.1]\n",
    "    n_features_values = [100, 500, 1000]\n",
    "    epochs = 6\n",
    "    \n",
    "    # Loop over parameter combinations\n",
    "    for C in C_values:\n",
    "            for lr in lr_values:\n",
    "                for batch_size in batch_size_values:\n",
    "                            print(f\"Training with C={C}, lr={lr}, batch_size={batch_size}\")\n",
    "                            \n",
    "                            # Train SVM using simple SGD\n",
    "                            w_linear = train_linear_svm(X_train, y_train, batch_size=batch_size, C=C, epochs=epochs, lr=lr)\n",
    "                            predictions_linear = predict(X_test, w_linear)\n",
    "                            print(predictions_linear)\n",
    "                            acc_linear = accuracy(y_test, predictions_linear)\n",
    "                            print(\"Accuracy of Linear SVM:\", acc_linear)\n",
    "\n",
    "                            # Train SVM using AdaGrad\n",
    "                            w_adagrad = train_svm_adagrad(X_train, y_train, batch_size=batch_size, C=C, epochs=epochs, lr=lr)\n",
    "                            predictions_adagrad = predict(X_test, w_adagrad)\n",
    "                            acc_adagrad = accuracy(y_test, predictions_adagrad)\n",
    "                            print(\"Accuracy of AdaGrad SVM:\", acc_adagrad)\n",
    "\n",
    "                            print(\"\\n\")\n",
    "                            \n",
    "    for C in C_values:\n",
    "            for lr in lr_values:\n",
    "                for batch_size in batch_size_values:\n",
    "                    for gamma in gamma_values:\n",
    "                        for n_features in n_features_values:\n",
    "                            print(f\"Training with C={C}, lr={lr}, batch_size={batch_size}, gamma={gamma}, n_features={n_features}\")\n",
    "                            # Train SVM using RFF\n",
    "                            w_rff, omega, b = train_svm_rff(X_train, y_train, gamma=gamma, n_features=n_features, C=C, epochs=epochs, lr=lr)\n",
    "                            predictions_rff = predict_rff(X_test, w_rff, omega, b)\n",
    "                            acc_rff = accuracy(y_test, predictions_rff)\n",
    "                            print(\"Accuracy of RFF SVM:\", acc_rff)\n",
    "\n",
    "                            print(\"\\n\")\n",
    "from sklearn.model_selection import KFold\n",
    "def main_cross():\n",
    "    # Load your dataset\n",
    "    X, y = load_imdb_dataset()\n",
    "    print(y)\n",
    "\n",
    "    # Parameters\n",
    "    C = 1.0\n",
    "    epochs = 1\n",
    "    lr = 0.01\n",
    "    batch_size = 10\n",
    "    gamma = 0.01\n",
    "    n_features = 300\n",
    "\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize lists to store performance metrics across folds\n",
    "    acc_linear_list = []\n",
    "    acc_adagrad_list = []\n",
    "    acc_rff_list = []\n",
    "\n",
    "    # Perform 5-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Train and evaluate Linear SVM\n",
    "        w_linear = train_linear_svm(X_train, y_train, batch_size=batch_size, C=C, epochs=epochs, lr=lr)\n",
    "        predictions_linear = predict(X_val, w_linear)\n",
    "        print(predictions_linear)\n",
    "        acc_linear = accuracy(y_val, predictions_linear)\n",
    "        acc_linear_list.append(acc_linear)\n",
    "\n",
    "        # Train and evaluate AdaGrad SVM\n",
    "        w_adagrad = train_svm_adagrad(X_train, y_train, batch_size=batch_size, C=C, epochs=epochs, lr=lr)\n",
    "        predictions_adagrad = predict(X_val, w_adagrad)\n",
    "        acc_adagrad = accuracy(y_val, predictions_adagrad)\n",
    "        acc_adagrad_list.append(acc_adagrad)\n",
    "\n",
    "        # Train and evaluate RFF SVM\n",
    "        w_rff, omega, b = train_svm_rff(X_train, y_train, gamma=gamma, n_features=n_features, C=C, epochs=epochs, lr=lr)\n",
    "        predictions_rff = predict_rff(X_val, w_rff, omega, b)\n",
    "        acc_rff = accuracy(y_val, predictions_rff)\n",
    "        acc_rff_list.append(acc_rff)\n",
    "\n",
    "    # Calculate average accuracy across all folds\n",
    "    avg_acc_linear = np.mean(acc_linear_list)\n",
    "    avg_acc_adagrad = np.mean(acc_adagrad_list)\n",
    "    avg_acc_rff = np.mean(acc_rff_list)\n",
    "\n",
    "    # Print the average accuracy for each SVM variant\n",
    "    print(\"Average Accuracy of Linear SVM:\", avg_acc_linear)\n",
    "    print(\"Average Accuracy of AdaGrad SVM:\", avg_acc_adagrad)\n",
    "    print(\"Average Accuracy of RFF SVM:\", avg_acc_rff)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef load_dataset(file_path):\\n    data = pd.read_csv(file_path)\\n    X = data.iloc[:, :-1].values\\n    y = data.iloc[:, -1].values\\n    scaler = StandardScaler()\\n    X = scaler.fit_transform(X)\\n    \\n    # print last 10 values of y\\n    print(\"Last 10 values of y: \", y[-10:])\\n    \\n    # print the shape of X and y\\n    print(\"X shape: \", X.shape)\\n    print(\"y shape: \", y.shape)\\n    return X, y\\n# Generate non-linearly separable data using make_moons\\ndef generate_non_linear_data(n_samples=100, noise=0.1):\\n    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=42)\\n    return X, y\\n# Load and standardize the dataset\\ndef load_dataset(file_path):\\n    data = pd.read_csv(file_path)\\n    X = data.iloc[:, :-1].values\\n    y = data.iloc[:, -1].values\\n    scaler = StandardScaler()\\n    X = scaler.fit_transform(X)\\n    return X, y\\n\\n# Train and evaluate a standard SVM\\ndef train_standard_svm(X, y, test_size=0.2, C=1.0, kernel=\\'linear\\'):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\\n    model = SVC(kernel=kernel, C=C)\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    return accuracy\\n\\n# Manual SVM implementation using gradient descent\\ndef hinge_loss_gradient(w, x, y, lambda_reg):\\n    # print shapes of w, x, y\\n\\n    if y * np.dot(w, x) < 1:\\n        return -y * x + lambda_reg * w\\n    return lambda_reg * w\\n\\n\\ndef train_manual_svm(X, y, epochs=1, eta=0.01, lambda_reg=0.01, batch_size=100):\\n    w = np.zeros(X.shape[1])\\n    for epoch in range(epochs):\\n        for i in range(0, X.shape[0], batch_size):\\n            end = i + batch_size\\n            X_batch, y_batch = X[i:end], y[i:end]\\n            for x, y_true in zip(X_batch, y_batch):\\n\\n                grad = hinge_loss_gradient(w, x, y_true, lambda_reg)\\n                w -= eta * grad\\n    return w\\n\\ndef evaluate_svm(X, y, w):\\n    predictions = np.sign(X @ w)\\n    accuracy = np.mean(predictions == y)\\n    return accuracy\\n\\n# SVM with Random Fourier Features (RFF)\\ndef random_fourier_features(X, n_features=300, gamma=1.0):\\n    \"\"\" Generate and apply Random Fourier Features for SVM. \"\"\"\\n    D = X.shape[1]\\n    omega = np.sqrt(2 * gamma) * np.random.randn(n_features, D)\\n    b = np.random.uniform(0, 2 * np.pi, n_features)\\n    X_rff = np.sqrt(2 / n_features) * np.cos(np.dot(X, omega.T) + b)\\n    return X_rff, omega, b\\n\\ndef train_and_evaluate_with_rff(X, y, n_features, gamma=1.0, C=1.0, eta=0.01, epochs=5, batch_size=10):\\n    X_rff = random_fourier_features(X, n_features, gamma)\\n    w = train_manual_svm(X_rff, y, epochs, eta, C, batch_size)\\n    accuracy = evaluate_svm(X_rff, y, w)\\n    return accuracy\\n\\ndef train_adagrad_svm(X, y, epochs=1, eta=1, lambda_reg=0.1, batch_size=10):\\n    w = np.zeros(X.shape[1])\\n    g_accumulate = np.zeros(X.shape[1])  # Gradient accumulation for Adagrad\\n    epsilon = 1e-8  # Smoothing term to avoid division by zero\\n    for epoch in range(epochs):\\n        for i in range(0, X.shape[0], batch_size):\\n            end = i + batch_size\\n            X_batch, y_batch = X[i:end], y[i:end]\\n            for x, y_true in zip(X_batch, y_batch):\\n                grad = hinge_loss_gradient(w, x, y_true, lambda_reg)\\n                g_accumulate += grad**2\\n                adjusted_eta = eta / (np.sqrt(g_accumulate) + epsilon)\\n                w -= adjusted_eta * grad\\n    return w\\ndef decision_function(X, w):\\n    return np.sign(np.dot(X, w))\\n\\n# Generate Random Fourier Features\\ndef generate_rff_parameters(n_features, D, gamma=1.0):\\n    omega = np.sqrt(2 * gamma) * np.random.randn(n_features, D)\\n    b = np.random.uniform(0, 2 * np.pi, n_features)\\n    return omega, b\\n\\ndef apply_rff(X, omega, b):\\n    X_rff = np.sqrt(2 / omega.shape[0]) * np.cos(np.dot(X, omega.T) + b)\\n    return X_rff\\n\\n\\n\\ndef plot_decision_boundary(X, y, model_func, title=\"Decision Boundary\", accuracy=None):\\n    \"\"\"\\n    Plot decision boundary for a 2D projection of the dataset and display accuracy.\\n    \\n    Parameters:\\n        X (np.array): The input features, can be more than 2D.\\n        y (np.array): The target variable (labels).\\n        model_func (function): A function that takes an array of inputs and returns model predictions.\\n        feature_indices (tuple): A tuple of indices to specify which two features to use for the plot.\\n        title (str): The plot title.\\n        accuracy (float): The accuracy of the model, if available.\\n    \"\"\"\\n    # Select two features based on indices\\n    # Default\\n    feature_indices=(0, 1)\\n    x_idx, y_idx = feature_indices\\n    x_min, x_max = X[:, x_idx].min() - 1, X[:, x_idx].max() + 1\\n    y_min, y_max = X[:, y_idx].min() - 1, X[:, y_idx].max() + 1\\n    h = 0.1  # step size in the mesh\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    # Prepare grid for model\\n    grid = np.c_[xx.ravel(), yy.ravel()]\\n    # Transform grid back to original feature space if necessary\\n    if X.shape[1] > 2:\\n        full_grid = np.zeros((grid.shape[0], X.shape[1]))\\n        full_grid[:, x_idx] = grid[:, 0]\\n        full_grid[:, y_idx] = grid[:, 1]\\n        grid = full_grid\\n    \\n    # Predict probabilities or class labels\\n    probs = model_func(grid).reshape(xx.shape)\\n    \\n    # Create plot\\n    plt.figure(figsize=(10, 6))\\n    plt.contourf(xx, yy, probs, 25, cmap=\"RdBu\", alpha=0.8)\\n    plt.scatter(X[:, x_idx], X[:, y_idx], c=y, cmap=\"RdBu\", edgecolors=\"k\")\\n    if accuracy is not None:\\n        plt.title(f\"{title} - Accuracy: {accuracy:.2%}\")\\n    else:\\n        plt.title(title)\\n    plt.xlabel(f\"Feature {x_idx + 1}\")\\n    plt.ylabel(f\"Feature {y_idx + 1}\")\\n    plt.colorbar()\\n    plt.show()\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\ndef train_standard_svm_with_cv(X, y, C=1.0, kernel=\\'linear\\', cv_folds=5):\\n    \"\"\"\\n    Train and evaluate a standard SVM using cross-validation.\\n    Parameters:\\n        X (np.array): Input features.\\n        y (np.array): Target labels.\\n        C (float): Regularization parameter.\\n        kernel (str): Kernel type of the SVM.\\n        cv_folds (int): Number of cross-validation folds.\\n    Returns:\\n        mean_accuracy (float): Mean accuracy across the cross-validation folds.\\n    \"\"\"\\n    model = SVC(kernel=kernel, C=C)\\n    accuracies = cross_val_score(model, X, y, cv=cv_folds, scoring=\\'accuracy\\')\\n    mean_accuracy = np.mean(accuracies)\\n    return mean_accuracy\\nfrom sklearn.model_selection import StratifiedKFold\\ndef cross_validate_svm(X, y, train_func, predict_func, params, cv_folds=5):\\n    \"\"\"\\n    Perform cross-validation for manually implemented SVM models.\\n    \\n    Parameters:\\n        X (np.array): Input features.\\n        y (np.array): Target labels.\\n        train_func (callable): Training function.\\n        predict_func (callable): Prediction function.\\n        params (dict): Parameters for the training function.\\n        cv_folds (int): Number of cross-validation folds.\\n        \\n    Returns:\\n        float: Mean accuracy across the cross-validation folds.\\n    \"\"\"\\n    kf = StratifiedKFold(n_splits=cv_folds)\\n    accuracies = []\\n    \\n    for train_index, test_index in kf.split(X, y):\\n        X_train, X_test = X[train_index], X[test_index]\\n        y_train, y_test = y[train_index], y[test_index]\\n        \\n        # Train the model\\n        model = train_func(X_train, y_train, **params)\\n        \\n        # Predict the results\\n        predictions = predict_func(X_test, model)\\n        \\n        # Calculate accuracy\\n        accuracy = accuracy_score(y_test, predictions)\\n        accuracies.append(accuracy)\\n    \\n    return np.mean(accuracies)\\ndef test_cross ():\\n    # Assume X, y are loaded properly\\n    X, y = generate_non_linear_data(100, 0.2)\\n\\n    # Parameters for Manual SVM\\n    for eta in [0.01, 0.05]:\\n        for lambda_reg in [0.01, 0.1]:\\n            params = {\\'epochs\\': 1, \\'eta\\': eta, \\'lambda_reg\\': lambda_reg}\\n            mean_accuracy = cross_validate_svm(X, y, train_manual_svm, lambda X, w: decision_function(X, w), params)\\n            print(f\"Manual SVM - eta: {eta}, lambda: {lambda_reg}, CV Accuracy: {mean_accuracy:.2f}\")\\n    \\n    # Parameters for Adagrad SVM\\n    for eta in [0.1, 0.5, 1.0]:\\n        for lambda_reg in [0.01, 0.05, 0.1]:\\n            params = {\\'epochs\\': 1, \\'eta\\': eta, \\'lambda_reg\\': lambda_reg}\\n            mean_accuracy = cross_validate_svm(X, y, train_adagrad_svm, lambda X, w: decision_function(X, w), params)\\n            print(f\"Adagrad SVM - eta: {eta}, lambda: {lambda_reg}, CV Accuracy: {mean_accuracy:.2f}\")\\n    \\n    # Parameters for SVM with Random Fourier Features\\n    for n_features in [100, 300]:\\n        for gamma in [0.5, 1.0]:\\n            params = {\\'n_features\\': n_features, \\'gamma\\': gamma}\\n            mean_accuracy = cross_validate_svm(X, y, train_and_evaluate_with_rff, lambda X, w: decision_function(apply_rff(X, omega, b), w_rff), params)\\n            print(f\"RFF SVM - Features: {n_features}, Gamma: {gamma}, CV Accuracy: {mean_accuracy:.2f}\")\\ndef test():\\n    #X, y = load_dataset(\"toydata_large.csv\")\\n    X, y = load_imdb_dataset(1000,100)\\n    # print shapes\\n    print(\"X shape: \", X.shape)\\n    print(\"y shape: \", y.shape)\\n    print(\"svm sttandard\", train_standard_svm_with_cv(X, y))\\n    # Parameters for Manual SVM\\n    for eta in [0.01, 0.05]:\\n        for lambda_reg in [0.01, 0.1]:\\n            w_manual = train_manual_svm(X, y, epochs=1, eta=eta, lambda_reg=lambda_reg)\\n            manual_predictions = decision_function(X, w_manual)\\n            manual_accuracy = accuracy_score(y, manual_predictions)\\n            print(f\"Manual SVM - eta: {eta}, lambda: {lambda_reg}, Accuracy: {manual_accuracy:.2f}\")\\n            #plot_decision_boundary(X, y, lambda x: decision_function(x, w_manual), f\"Manual SVM - eta: {eta}, lambda: {lambda_reg}\", manual_accuracy)\\n    for eta in [0.1, 0.5, 1.0]:\\n        \\n        for lambda_reg in [0.01, 0.05, 0.1]:\\n            w_adagrad = train_adagrad_svm(X, y, epochs=1, eta=eta, lambda_reg=lambda_reg)\\n            adagrad_predictions = decision_function(X, w_adagrad)\\n            adagrad_accuracy = accuracy_score(y, adagrad_predictions)\\n            print(f\"Adagrad SVM - eta: {eta}, lambda: {lambda_reg}, Accuracy: {adagrad_accuracy:.2f}\")\\n            #plot_decision_boundary(X, y, lambda x: decision_function(x, w_adagrad), f\"Adagrad SVM - eta: {eta}, lambda: {lambda_reg}\", adagrad_accuracy)\\n    # Parameters for SVM with Random Fourier Features\\n    for n_features in [100, 300]:\\n        for gamma in [0.5, 1.0]:\\n            X_rff, omega, b = random_fourier_features(X, n_features, gamma)\\n            w_rff = train_manual_svm(X_rff, y)\\n            rff_predictions = decision_function(X_rff, w_rff)\\n            rff_accuracy = accuracy_score(y, rff_predictions)\\n            print(f\"RFF SVM - Features: {n_features}, Gamma: {gamma}, Accuracy: {rff_accuracy:.2f}\")\\n            #plot_decision_boundary(X, y, lambda x: decision_function(apply_rff(x, omega, b), w_rff), f\"RFF SVM - Features: {n_features}, Gamma: {gamma}\", rff_accuracy)\\n\\n# Main execution\\nif __name__ == \"__main__\":\\n    if False:\\n        X, y = load_dataset(\"toydata_tiny.csv\")\\n\\n        # SVM with sklearn\\n        clf = SVC(kernel=\\'linear\\')\\n        clf.fit(X, y)\\n        y_pred = clf.predict(X)\\n        accuracy = accuracy_score(y, y_pred)\\n        plot_decision_boundary(X, y, lambda x: clf.predict(x), \"Standard SVM\", accuracy)\\n\\n        # Manual SVM - Assuming weights are trained and available as \\'w_manual\\'\\n        # For demonstration, use random weights\\n        w_manual = np.random.randn(X.shape[1])\\n        manual_predictions = decision_function(X, w_manual)\\n        manual_accuracy = accuracy_score(y, manual_predictions)\\n        plot_decision_boundary(X, y, lambda x: decision_function(x, w_manual), \"Manual SVM\", manual_accuracy)\\n\\n        # Adagrad SVM - Assuming weights are trained and available as \\'w_adagrad\\'\\n        w_adagrad = np.random.randn(X.shape[1])\\n        adagrad_predictions = decision_function(X, w_adagrad)\\n        adagrad_accuracy = accuracy_score(y, adagrad_predictions)\\n        plot_decision_boundary(X, y, lambda x: decision_function(x, w_adagrad), \"Adagrad SVM\", adagrad_accuracy)\\n\\n        # SVM with Random Fourier Features\\n        X_rff, omega, b = random_fourier_features(X)\\n        w_rff = train_manual_svm(X_rff, y)\\n        rff_predictions = decision_function(X_rff, w_rff)\\n        rff_accuracy = accuracy_score(y, rff_predictions)\\n        plot_decision_boundary(X, y, lambda x: decision_function(apply_rff(x, omega, b), w_rff), \"RFF SVM\", rff_accuracy)\\n    else:\\n        test()\\n    \\n    \\n    \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best old version\n",
    "\n",
    "'''\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # print last 10 values of y\n",
    "    print(\"Last 10 values of y: \", y[-10:])\n",
    "    \n",
    "    # print the shape of X and y\n",
    "    print(\"X shape: \", X.shape)\n",
    "    print(\"y shape: \", y.shape)\n",
    "    return X, y\n",
    "# Generate non-linearly separable data using make_moons\n",
    "def generate_non_linear_data(n_samples=100, noise=0.1):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "    return X, y\n",
    "# Load and standardize the dataset\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Train and evaluate a standard SVM\n",
    "def train_standard_svm(X, y, test_size=0.2, C=1.0, kernel='linear'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    model = SVC(kernel=kernel, C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Manual SVM implementation using gradient descent\n",
    "def hinge_loss_gradient(w, x, y, lambda_reg):\n",
    "    # print shapes of w, x, y\n",
    "\n",
    "    if y * np.dot(w, x) < 1:\n",
    "        return -y * x + lambda_reg * w\n",
    "    return lambda_reg * w\n",
    "\n",
    "\n",
    "def train_manual_svm(X, y, epochs=1, eta=0.01, lambda_reg=0.01, batch_size=100):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            end = i + batch_size\n",
    "            X_batch, y_batch = X[i:end], y[i:end]\n",
    "            for x, y_true in zip(X_batch, y_batch):\n",
    "\n",
    "                grad = hinge_loss_gradient(w, x, y_true, lambda_reg)\n",
    "                w -= eta * grad\n",
    "    return w\n",
    "\n",
    "def evaluate_svm(X, y, w):\n",
    "    predictions = np.sign(X @ w)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    return accuracy\n",
    "\n",
    "# SVM with Random Fourier Features (RFF)\n",
    "def random_fourier_features(X, n_features=300, gamma=1.0):\n",
    "    \"\"\" Generate and apply Random Fourier Features for SVM. \"\"\"\n",
    "    D = X.shape[1]\n",
    "    omega = np.sqrt(2 * gamma) * np.random.randn(n_features, D)\n",
    "    b = np.random.uniform(0, 2 * np.pi, n_features)\n",
    "    X_rff = np.sqrt(2 / n_features) * np.cos(np.dot(X, omega.T) + b)\n",
    "    return X_rff, omega, b\n",
    "\n",
    "def train_and_evaluate_with_rff(X, y, n_features, gamma=1.0, C=1.0, eta=0.01, epochs=5, batch_size=10):\n",
    "    X_rff = random_fourier_features(X, n_features, gamma)\n",
    "    w = train_manual_svm(X_rff, y, epochs, eta, C, batch_size)\n",
    "    accuracy = evaluate_svm(X_rff, y, w)\n",
    "    return accuracy\n",
    "\n",
    "def train_adagrad_svm(X, y, epochs=1, eta=1, lambda_reg=0.1, batch_size=10):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    g_accumulate = np.zeros(X.shape[1])  # Gradient accumulation for Adagrad\n",
    "    epsilon = 1e-8  # Smoothing term to avoid division by zero\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            end = i + batch_size\n",
    "            X_batch, y_batch = X[i:end], y[i:end]\n",
    "            for x, y_true in zip(X_batch, y_batch):\n",
    "                grad = hinge_loss_gradient(w, x, y_true, lambda_reg)\n",
    "                g_accumulate += grad**2\n",
    "                adjusted_eta = eta / (np.sqrt(g_accumulate) + epsilon)\n",
    "                w -= adjusted_eta * grad\n",
    "    return w\n",
    "def decision_function(X, w):\n",
    "    return np.sign(np.dot(X, w))\n",
    "\n",
    "# Generate Random Fourier Features\n",
    "def generate_rff_parameters(n_features, D, gamma=1.0):\n",
    "    omega = np.sqrt(2 * gamma) * np.random.randn(n_features, D)\n",
    "    b = np.random.uniform(0, 2 * np.pi, n_features)\n",
    "    return omega, b\n",
    "\n",
    "def apply_rff(X, omega, b):\n",
    "    X_rff = np.sqrt(2 / omega.shape[0]) * np.cos(np.dot(X, omega.T) + b)\n",
    "    return X_rff\n",
    "\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X, y, model_func, title=\"Decision Boundary\", accuracy=None):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for a 2D projection of the dataset and display accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.array): The input features, can be more than 2D.\n",
    "        y (np.array): The target variable (labels).\n",
    "        model_func (function): A function that takes an array of inputs and returns model predictions.\n",
    "        feature_indices (tuple): A tuple of indices to specify which two features to use for the plot.\n",
    "        title (str): The plot title.\n",
    "        accuracy (float): The accuracy of the model, if available.\n",
    "    \"\"\"\n",
    "    # Select two features based on indices\n",
    "    # Default\n",
    "    feature_indices=(0, 1)\n",
    "    x_idx, y_idx = feature_indices\n",
    "    x_min, x_max = X[:, x_idx].min() - 1, X[:, x_idx].max() + 1\n",
    "    y_min, y_max = X[:, y_idx].min() - 1, X[:, y_idx].max() + 1\n",
    "    h = 0.1  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Prepare grid for model\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # Transform grid back to original feature space if necessary\n",
    "    if X.shape[1] > 2:\n",
    "        full_grid = np.zeros((grid.shape[0], X.shape[1]))\n",
    "        full_grid[:, x_idx] = grid[:, 0]\n",
    "        full_grid[:, y_idx] = grid[:, 1]\n",
    "        grid = full_grid\n",
    "    \n",
    "    # Predict probabilities or class labels\n",
    "    probs = model_func(grid).reshape(xx.shape)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, probs, 25, cmap=\"RdBu\", alpha=0.8)\n",
    "    plt.scatter(X[:, x_idx], X[:, y_idx], c=y, cmap=\"RdBu\", edgecolors=\"k\")\n",
    "    if accuracy is not None:\n",
    "        plt.title(f\"{title} - Accuracy: {accuracy:.2%}\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    plt.xlabel(f\"Feature {x_idx + 1}\")\n",
    "    plt.ylabel(f\"Feature {y_idx + 1}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "def train_standard_svm_with_cv(X, y, C=1.0, kernel='linear', cv_folds=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate a standard SVM using cross-validation.\n",
    "    Parameters:\n",
    "        X (np.array): Input features.\n",
    "        y (np.array): Target labels.\n",
    "        C (float): Regularization parameter.\n",
    "        kernel (str): Kernel type of the SVM.\n",
    "        cv_folds (int): Number of cross-validation folds.\n",
    "    Returns:\n",
    "        mean_accuracy (float): Mean accuracy across the cross-validation folds.\n",
    "    \"\"\"\n",
    "    model = SVC(kernel=kernel, C=C)\n",
    "    accuracies = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    return mean_accuracy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cross_validate_svm(X, y, train_func, predict_func, params, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for manually implemented SVM models.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.array): Input features.\n",
    "        y (np.array): Target labels.\n",
    "        train_func (callable): Training function.\n",
    "        predict_func (callable): Prediction function.\n",
    "        params (dict): Parameters for the training function.\n",
    "        cv_folds (int): Number of cross-validation folds.\n",
    "        \n",
    "    Returns:\n",
    "        float: Mean accuracy across the cross-validation folds.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits=cv_folds)\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model = train_func(X_train, y_train, **params)\n",
    "        \n",
    "        # Predict the results\n",
    "        predictions = predict_func(X_test, model)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return np.mean(accuracies)\n",
    "def test_cross ():\n",
    "    # Assume X, y are loaded properly\n",
    "    X, y = generate_non_linear_data(100, 0.2)\n",
    "\n",
    "    # Parameters for Manual SVM\n",
    "    for eta in [0.01, 0.05]:\n",
    "        for lambda_reg in [0.01, 0.1]:\n",
    "            params = {'epochs': 1, 'eta': eta, 'lambda_reg': lambda_reg}\n",
    "            mean_accuracy = cross_validate_svm(X, y, train_manual_svm, lambda X, w: decision_function(X, w), params)\n",
    "            print(f\"Manual SVM - eta: {eta}, lambda: {lambda_reg}, CV Accuracy: {mean_accuracy:.2f}\")\n",
    "    \n",
    "    # Parameters for Adagrad SVM\n",
    "    for eta in [0.1, 0.5, 1.0]:\n",
    "        for lambda_reg in [0.01, 0.05, 0.1]:\n",
    "            params = {'epochs': 1, 'eta': eta, 'lambda_reg': lambda_reg}\n",
    "            mean_accuracy = cross_validate_svm(X, y, train_adagrad_svm, lambda X, w: decision_function(X, w), params)\n",
    "            print(f\"Adagrad SVM - eta: {eta}, lambda: {lambda_reg}, CV Accuracy: {mean_accuracy:.2f}\")\n",
    "    \n",
    "    # Parameters for SVM with Random Fourier Features\n",
    "    for n_features in [100, 300]:\n",
    "        for gamma in [0.5, 1.0]:\n",
    "            params = {'n_features': n_features, 'gamma': gamma}\n",
    "            mean_accuracy = cross_validate_svm(X, y, train_and_evaluate_with_rff, lambda X, w: decision_function(apply_rff(X, omega, b), w_rff), params)\n",
    "            print(f\"RFF SVM - Features: {n_features}, Gamma: {gamma}, CV Accuracy: {mean_accuracy:.2f}\")\n",
    "def test():\n",
    "    #X, y = load_dataset(\"toydata_large.csv\")\n",
    "    X, y = load_imdb_dataset(1000,100)\n",
    "    # print shapes\n",
    "    print(\"X shape: \", X.shape)\n",
    "    print(\"y shape: \", y.shape)\n",
    "    print(\"svm sttandard\", train_standard_svm_with_cv(X, y))\n",
    "    # Parameters for Manual SVM\n",
    "    for eta in [0.01, 0.05]:\n",
    "        for lambda_reg in [0.01, 0.1]:\n",
    "            w_manual = train_manual_svm(X, y, epochs=1, eta=eta, lambda_reg=lambda_reg)\n",
    "            manual_predictions = decision_function(X, w_manual)\n",
    "            manual_accuracy = accuracy_score(y, manual_predictions)\n",
    "            print(f\"Manual SVM - eta: {eta}, lambda: {lambda_reg}, Accuracy: {manual_accuracy:.2f}\")\n",
    "            #plot_decision_boundary(X, y, lambda x: decision_function(x, w_manual), f\"Manual SVM - eta: {eta}, lambda: {lambda_reg}\", manual_accuracy)\n",
    "    for eta in [0.1, 0.5, 1.0]:\n",
    "        \n",
    "        for lambda_reg in [0.01, 0.05, 0.1]:\n",
    "            w_adagrad = train_adagrad_svm(X, y, epochs=1, eta=eta, lambda_reg=lambda_reg)\n",
    "            adagrad_predictions = decision_function(X, w_adagrad)\n",
    "            adagrad_accuracy = accuracy_score(y, adagrad_predictions)\n",
    "            print(f\"Adagrad SVM - eta: {eta}, lambda: {lambda_reg}, Accuracy: {adagrad_accuracy:.2f}\")\n",
    "            #plot_decision_boundary(X, y, lambda x: decision_function(x, w_adagrad), f\"Adagrad SVM - eta: {eta}, lambda: {lambda_reg}\", adagrad_accuracy)\n",
    "    # Parameters for SVM with Random Fourier Features\n",
    "    for n_features in [100, 300]:\n",
    "        for gamma in [0.5, 1.0]:\n",
    "            X_rff, omega, b = random_fourier_features(X, n_features, gamma)\n",
    "            w_rff = train_manual_svm(X_rff, y)\n",
    "            rff_predictions = decision_function(X_rff, w_rff)\n",
    "            rff_accuracy = accuracy_score(y, rff_predictions)\n",
    "            print(f\"RFF SVM - Features: {n_features}, Gamma: {gamma}, Accuracy: {rff_accuracy:.2f}\")\n",
    "            #plot_decision_boundary(X, y, lambda x: decision_function(apply_rff(x, omega, b), w_rff), f\"RFF SVM - Features: {n_features}, Gamma: {gamma}\", rff_accuracy)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    if False:\n",
    "        X, y = load_dataset(\"toydata_tiny.csv\")\n",
    "\n",
    "        # SVM with sklearn\n",
    "        clf = SVC(kernel='linear')\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        plot_decision_boundary(X, y, lambda x: clf.predict(x), \"Standard SVM\", accuracy)\n",
    "\n",
    "        # Manual SVM - Assuming weights are trained and available as 'w_manual'\n",
    "        # For demonstration, use random weights\n",
    "        w_manual = np.random.randn(X.shape[1])\n",
    "        manual_predictions = decision_function(X, w_manual)\n",
    "        manual_accuracy = accuracy_score(y, manual_predictions)\n",
    "        plot_decision_boundary(X, y, lambda x: decision_function(x, w_manual), \"Manual SVM\", manual_accuracy)\n",
    "\n",
    "        # Adagrad SVM - Assuming weights are trained and available as 'w_adagrad'\n",
    "        w_adagrad = np.random.randn(X.shape[1])\n",
    "        adagrad_predictions = decision_function(X, w_adagrad)\n",
    "        adagrad_accuracy = accuracy_score(y, adagrad_predictions)\n",
    "        plot_decision_boundary(X, y, lambda x: decision_function(x, w_adagrad), \"Adagrad SVM\", adagrad_accuracy)\n",
    "\n",
    "        # SVM with Random Fourier Features\n",
    "        X_rff, omega, b = random_fourier_features(X)\n",
    "        w_rff = train_manual_svm(X_rff, y)\n",
    "        rff_predictions = decision_function(X_rff, w_rff)\n",
    "        rff_accuracy = accuracy_score(y, rff_predictions)\n",
    "        plot_decision_boundary(X, y, lambda x: decision_function(apply_rff(x, omega, b), w_rff), \"RFF SVM\", rff_accuracy)\n",
    "    else:\n",
    "        test()\n",
    "    \n",
    "    \n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
