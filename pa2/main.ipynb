{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b18ccbf9d81695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T10:08:35.423177Z",
     "start_time": "2024-05-06T10:08:35.413972Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Third-party imports for data manipulation and calculation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and data processing libraries\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Third-party utilities\n",
    "from tqdm import tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e95d8210",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "The code is in the first part at the end of the file you can find the report and some specific code for the report but the most important code is in the front"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947ed131c9928ad",
   "metadata": {},
   "source": [
    "# Mining Massive Data (SS2024)\n",
    "## Programming Assignment 2 (Large-scale SVM Training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e38e6a16c4eb8",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "In this part we have the functions\n",
    "\n",
    "to load the regular dataset\n",
    "\n",
    "to load the imdb dataset \n",
    "\n",
    "to load the imdb dataset and drop 50% of the features as whished in the report later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafd086d100916b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:47:34.535347Z",
     "start_time": "2024-05-06T07:47:34.511406Z"
    }
   },
   "source": [
    "# helper functions\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # print last 10 values of y\n",
    "    #print(\"Last 10 values of y: \", y[-10:])\n",
    "    \n",
    "    # print the shape of X and y\n",
    "    print(\"X shape: \", X.shape)\n",
    "    print(\"y shape: \", y.shape)\n",
    "    return X, y\n",
    "\n",
    "def load_imdb_dataset_with_dropout(num_samples=100, max_length=500, dropout_prob=0.5):\n",
    "    # Load the dataset\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "        path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None,\n",
    "        seed=113, start_char=1, oov_char=2, index_from=3\n",
    "    )\n",
    "\n",
    "    # Cut the dataset to the specified number of samples\n",
    "    x_train = x_train[:num_samples]\n",
    "    y_train = y_train[:num_samples]\n",
    "\n",
    "    # Pad the sequences to have uniform length\n",
    "    x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Print the shape of the dataset\n",
    "    print(\"Shape of x_train:\", x_train.shape)\n",
    "\n",
    "    # Apply feature dropout\n",
    "    # Generate a random mask of the same shape as x_train\n",
    "    mask = np.random.rand(*x_train.shape) < (1 - dropout_prob)\n",
    "    # Apply the mask to x_train to drop some features\n",
    "    x_train = x_train * mask\n",
    "    return x_train, y_train\n",
    "    \n",
    "    \n",
    "def load_imdb_dataset(num_samples=100, max_length=500):\n",
    "    # Load the dataset\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3)\n",
    "    x_train = x_train[:num_samples]\n",
    "    y_train = y_train[:num_samples]\n",
    "    # create np array of both \n",
    "    x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "    print(\"Shape of x_train: \", x_train.shape)\n",
    "    y_train = np.array(y_train)\n",
    "    return x_train, y_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9d1d53208c964f28",
   "metadata": {},
   "source": [
    "### Toydata\n",
    "\n",
    "first take a look how the toydata_tiny looks like between feature 1 and 2 and as we can see the svm should get a pretty good result here as the data is good seperable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7b0c489514064d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:49:00.744016Z",
     "start_time": "2024-05-06T07:49:00.159812Z"
    }
   },
   "source": [
    "#plot X and y values\n",
    "X_tiny, y_tiny = load_dataset(\"toydata_tiny.csv\")\n",
    "plt.scatter(X_tiny[:, 0], X_tiny[:, 1], c=y_tiny, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Toy Data')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d016bad1",
   "metadata": {},
   "source": [
    "here is also a plot for the toydata large which has more features but the data still looks pretty clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dcb6895074db38b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:50:09.499251Z",
     "start_time": "2024-05-06T07:50:04.444783Z"
    }
   },
   "source": [
    "X_large, y_large = load_dataset(\"toydata_large.csv\")\n",
    "plt.scatter(X_large[:, 2], X_large[:, 3], c=y_large, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Toy Data')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd07f1810a6a9f0",
   "metadata": {},
   "source": [
    "### Imdbdata\n",
    "\n",
    "Here the imdb data is loaded and plottet between two features andd as we can see this looks pretty bad for a svm to get much accuracy here, so the excepctations should be much lower in comparison to the toydata here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81ae41f4aad0ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:39:10.925974Z",
     "start_time": "2024-05-06T09:39:07.149094Z"
    }
   },
   "source": [
    "# Usage\n",
    "X_imdb, y_imdb = load_imdb_dataset(1000,2)\n",
    "\n",
    "# plot the features of X to y value (0/1)\n",
    "plt.scatter(X_imdb[:, 0], X_imdb[:, 1], c=y_imdb, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('IMDB Data')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "28e7557090ac2063",
   "metadata": {},
   "source": [
    "### Preprocessing(optional)\n",
    "\n",
    "this is some preprocessing we tested in order to improve the accurcay of the imdb datasets it uses a lot of libaries and some different techniques, however it really increased the performance of the svm for the imdb dataset and madde it much better\n",
    "\n",
    "you can try it out by using load_and_preprocess_imdb in the best parameter search for example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3b876025a82d60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T07:50:01.698233Z",
     "start_time": "2024-05-04T07:50:01.688125Z"
    }
   },
   "source": [
    "'''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def select_features(X, y, k=500):\n",
    "    # Apply Chi-squared test to select the top k features\n",
    "    chi2_selector = SelectKBest(chi2, k=k)\n",
    "    X_kbest_features = chi2_selector.fit_transform(X, y)\n",
    "    return X_kbest_features\n",
    "\n",
    "def apply_lda(X, y):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    X_lda = lda.fit_transform(X, y)\n",
    "    return X_lda\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(X, y):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(X[:, 0], np.zeros_like(X[:, 0]), c=y, cmap='coolwarm', alpha=0.5)\n",
    "    plt.colorbar()  # Show color scale\n",
    "    plt.xlabel('Projected Feature')\n",
    "    plt.ylabel('Dummy Zero Axis')\n",
    "    plt.title('IMDB Data Projection via LDA')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_imdb(num_samples=1000, max_length=500, num_features=10000, top_k_features=500):\n",
    "    # Load the IMDB dataset\n",
    "    (x_train, y_train), _ = imdb.load_data(num_words=num_features)\n",
    "    x_train = x_train[:num_samples]\n",
    "    y_train = y_train[:num_samples]\n",
    "\n",
    "    # Pad the sequences to ensure they are of equal length\n",
    "    x_train_padded = pad_sequences(x_train, maxlen=max_length)\n",
    "\n",
    "    # Convert the integer sequences to strings (for TF-IDF)\n",
    "    # Each token (integer) is converted directly to a string\n",
    "    x_train_text = [\" \".join(map(str, seq)) for seq in x_train_padded]\n",
    "\n",
    "    # Initialize and apply a TF-IDF transformation\n",
    "    tfidf = TfidfVectorizer(max_features=num_features, dtype=np.float32)\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train_text)\n",
    "\n",
    "    # Convert the sparse TF-IDF matrix to a dense format\n",
    "    x_train_tfidf_dense = x_train_tfidf.toarray()\n",
    "\n",
    "    # Select the top k features based on the chi-squared statistic\n",
    "    selector = SelectKBest(chi2, k=top_k_features)\n",
    "    x_train_kbest = selector.fit_transform(x_train_tfidf_dense, y_train)\n",
    "\n",
    "    # Apply Linear Discriminant Analysis (LDA) for dimensionality reduction\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    x_train_lda = lda.fit_transform(x_train_kbest, y_train)\n",
    "    \n",
    "    return x_train_lda, y_train\n",
    "\n",
    "# Load and preprocess the data\n",
    "X_processed, y_processed = load_and_preprocess_imdb()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the SVM model on the test set: {accuracy:.2f}\")\n",
    "\n",
    "# Optional: Visualization of decision boundary\n",
    "plt.scatter(X_processed[:, 0], np.zeros_like(X_processed[:, 0]), c=y_processed, cmap='coolwarm', alpha=0.5)\n",
    "plt.scatter(X_test[:, 0], np.zeros_like(X_test[:, 0]), c=y_pred, marker='x')\n",
    "plt.colorbar()  # Show color scale\n",
    "plt.xlabel('Projected Feature by LDA')\n",
    "plt.ylabel('Dummy Zero Axis')\n",
    "plt.title('IMDB Data Projection and SVM Decision Boundary')\n",
    "plt.show()'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4c5401b40a032cd9",
   "metadata": {},
   "source": [
    "## Evaluation methods\n",
    "These methods are basic methods for the svms and are used from all implementations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f9d26997be2cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T20:35:44.045795Z",
     "start_time": "2024-05-05T20:35:44.034938Z"
    }
   },
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict using the trained weights of the SVM.\n",
    "    - X: feature matrix\n",
    "    - w: weight vector\n",
    "    \"\"\"\n",
    "    return np.sign(np.dot(X, w))\n",
    "\n",
    "def predict_rff(X, w, omega, b):\n",
    "    \"\"\"\n",
    "    Predict with the SVM trained on RFF-transformed data.\n",
    "    - X: original feature matrix\n",
    "    - w: trained weight vector\n",
    "    - omega: frequencies matrix used for RFF\n",
    "    - b: biases used for RFF\n",
    "    \"\"\"\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    return np.sign(np.dot(X_transformed, w))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions.\n",
    "    - y_true: true labels\n",
    "    - y_pred: predicted labels\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ae5f1dbb",
   "metadata": {},
   "source": [
    "## Parameter evaluation\n",
    "\n",
    "These functions take a couple of models and the datasets to find the best ones\n",
    "\n",
    "`evaluate_models_k` uses 5 cross validation and is used for toydata\n",
    "`evaluate_models` is just regular evaluationm without 5 cross validation\n",
    "\n",
    "This methods help to find the optimal parameters please look a bit down for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ab1e2111f9a7bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T20:35:54.704062Z",
     "start_time": "2024-05-05T20:35:54.672299Z"
    }
   },
   "source": [
    "def evaluate_models_k(datasets, model_types, params):\n",
    "    \"\"\"\n",
    "    Evaluates different model types on datasets with various parameter combinations using 5-fold cross-validation.\n",
    "\n",
    "    Args:\n",
    "        datasets: A dictionary with dataset names as keys and (X, y) tuples as values.\n",
    "        model_types: A list of strings denoting the types of models to evaluate.\n",
    "        params: A dictionary with model types as keys and lists of parameter combinations as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for name, (X, y) in datasets.items():\n",
    "        results[name] = {}\n",
    "        if name == 'imdb':\n",
    "            y = np.where(y == 0, -1, 1)  # Convert labels\n",
    "\n",
    "        for model_type in model_types:\n",
    "            best_accuracy = 0\n",
    "            best_params = {}\n",
    "\n",
    "            param_combinations = params[model_type]\n",
    "            progress_bar = tqdm(param_combinations, desc=f\"Processing {name} - {model_type}\")\n",
    "\n",
    "            for param_comb in progress_bar:\n",
    "                # Initialize KFold with 5 splits\n",
    "                kf = KFold(n_splits=5)\n",
    "\n",
    "                fold_accuracies = []\n",
    "\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    # Split the data into training and testing sets\n",
    "                    X_train, X_test = X[train_index], X[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                    # Train model\n",
    "                    if model_type == 'linear':\n",
    "                        w = train_svm_linear(X_train, y_train, **param_comb)\n",
    "                        predictions = predict(X_test, w)\n",
    "                    elif model_type == 'adagrad':\n",
    "                        w = train_svm_adagrad(X_train, y_train, **param_comb)\n",
    "                        predictions = predict(X_test, w)\n",
    "                    elif model_type == 'rff':\n",
    "                        w, omega, b = train_svm_rff(X_train, y_train, **param_comb)\n",
    "                        predictions = predict_rff(X_test, w, omega, b)\n",
    "\n",
    "                    acc = accuracy(y_test, predictions)\n",
    "                    fold_accuracies.append(acc)\n",
    "\n",
    "                # Average accuracy over all folds\n",
    "                avg_acc = np.mean(fold_accuracies)\n",
    "                progress_bar.set_postfix_str(f\"Acc: {avg_acc:.4f}\")\n",
    "\n",
    "                # Update if better\n",
    "                if avg_acc > best_accuracy:\n",
    "                    best_accuracy = avg_acc\n",
    "                    best_params = param_comb\n",
    "\n",
    "            results[name][model_type] = {\n",
    "                'Best Parameters': best_params,\n",
    "                'Best Accuracy': best_accuracy\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_models(datasets, model_types, params):\n",
    "    \"\"\"\n",
    "    Evaluates different model types on datasets with various parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        datasets: A dictionary with dataset names as keys and (X, y) tuples as values.\n",
    "        model_types: A list of strings denoting the types of models to evaluate.\n",
    "        params: A dictionary with model types as keys and lists of parameter combinations as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for name, (X, y) in datasets.items():\n",
    "        results[name] = {}\n",
    "        if name == 'imdb':\n",
    "            y = np.where(y == 0, -1, 1)  # Convert labels\n",
    "\n",
    "        for model_type in model_types:\n",
    "            best_accuracy = 0\n",
    "            best_params = {}\n",
    "\n",
    "            param_combinations = params[model_type]\n",
    "            progress_bar = tqdm(param_combinations, desc=f\"Processing {name} - {model_type}\")\n",
    "\n",
    "            for param_comb in progress_bar:\n",
    "                # Train model\n",
    "                if model_type == 'linear':\n",
    "                    w = train_svm_linear(X, y, **param_comb)\n",
    "                    predictions = predict(X, w)\n",
    "                elif model_type == 'adagrad':\n",
    "                    w = train_svm_linear(X, y, **param_comb)\n",
    "                    predictions = predict(X, w)\n",
    "                elif model_type == 'rff':\n",
    "                    w, omega, b = train_svm_rff(X, y, **param_comb)\n",
    "                    predictions = predict_rff(X, w, omega, b)\n",
    "\n",
    "                # Evaluate accuracy\n",
    "                avg_acc = accuracy(y, predictions)\n",
    "                progress_bar.set_postfix_str(f\"Acc: {avg_acc:.4f}\")\n",
    "\n",
    "                # Update if better\n",
    "                if avg_acc > best_accuracy:\n",
    "                    best_accuracy = avg_acc\n",
    "                    best_params = param_comb\n",
    "\n",
    "            results[name][model_type] = {\n",
    "                'Best Parameters': best_params,\n",
    "                'Best Accuracy': best_accuracy\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d62dd199eeb6f48e",
   "metadata": {},
   "source": [
    "### Linear SVM Model\n",
    "\n",
    "This is the code for task a thze regular svm model with bach training and its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a083ae338126ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T11:31:53.564598Z",
     "start_time": "2024-05-06T11:31:53.548412Z"
    }
   },
   "source": [
    "def svm_loss(W, X, y, C):\n",
    "    distances = 1 - y * np.dot(X, W)\n",
    "    hinge_loss = C * np.mean(np.maximum(0, distances))\n",
    "    regularization_loss = 0.5 * np.dot(W, W)\n",
    "    return regularization_loss + hinge_loss\n",
    "\n",
    "def svm_gradient(W, X_batch, y_batch, C):\n",
    "    distances = 1 - y_batch * np.dot(X_batch, W)\n",
    "    dw = np.zeros_like(W)\n",
    "    for ind, d in enumerate(distances):\n",
    "        if d > 0:\n",
    "            # Only the misclassified points affect the gradient\n",
    "            dw += C * (-y_batch[ind] * X_batch[ind])\n",
    "    dw /= len(y_batch)  # Average over the batch\n",
    "    dw += W  # Add gradient of the regularization term\n",
    "    return dw\n",
    "\n",
    "def train_svm_linear(X, y, batch_size=10, C=1.0, epochs=10, lr=0.01):\n",
    "    w = np.random.randn(X.shape[1]) / np.sqrt(X.shape[1])  # Improved initialization\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(len(y))  # Shuffle the data\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(start + batch_size, len(y))\n",
    "            X_batch = X[perm[start:end]]\n",
    "            y_batch = y[perm[start:end]]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            w -= lr * grad\n",
    "        # Optionally, decrease learning rate gradually\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr /= 2\n",
    "    return w\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "84a9feb397eeac2f",
   "metadata": {},
   "source": [
    "### Optimization using Adagrad\n",
    "\n",
    "This is the code for task b the adagrad svm model with bach training and its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae057e86309a9c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T11:37:33.582595Z",
     "start_time": "2024-05-06T11:37:33.571429Z"
    }
   },
   "source": [
    "def train_svm_adagrad(X, y, batch_size=10, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a linear SVM using AdaGrad optimization.\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    - batch_size: size of the mini-batch\n",
    "    - C: regularization strength\n",
    "    - epochs: number of training epochs\n",
    "    - lr: initial learning rate\n",
    "    \"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    epsilon = 1e-8  # smoothing term to avoid division by zero\n",
    "    gradient_accumulate = np.zeros(X.shape[1]) # saves the past gradients squared\n",
    "    n_batches = int(len(y) / batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            gradient_accumulate += grad ** 2\n",
    "            adjusted_lr = lr / (epsilon + np.sqrt(gradient_accumulate)) #adjusts learning rate depenending on gradient from before \n",
    "            w = w - adjusted_lr * grad\n",
    "            \n",
    "    return w\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfb8039346228ce",
   "metadata": {},
   "source": [
    "### Random Fourier Features\n",
    "\n",
    "This is the code for task c the Random Fourier Features svm model with bach training and its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19fc30225bdc0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T11:36:42.526722Z",
     "start_time": "2024-05-06T11:36:42.509043Z"
    }
   },
   "source": [
    "def rff_gaussian_transform(X, omega, b):\n",
    "    \"\"\"\n",
    "    Apply the Random Fourier Features transformation to the dataset.\n",
    "    - X: feature matrix\n",
    "    - omega: frequencies matrix\n",
    "    - b: bias terms\n",
    "    \"\"\"\n",
    "    X_features = np.dot(X, omega) + b\n",
    "    return np.cos(X_features)\n",
    "\n",
    "def train_svm_rff(X, y,batch_size=10, gamma= 0.01, n_features=500, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a SVM with RFF approximation for a Gaussian kernel.\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    - gamma: parameter of Gaussian kernel\n",
    "    - n_features: number of random features\n",
    "    - C: regularization strength\n",
    "    - epochs: number of training epochs\n",
    "    - lr: learning rate\n",
    "    \"\"\"\n",
    "    # Create Random Fourier Features transformation parameters\n",
    "    omega = np.random.normal(scale=2*np.pi*gamma, size=(X.shape[1], n_features))\n",
    "    b = np.random.uniform(0, 2*np.pi, size=n_features)\n",
    "    # random funktions that approximate ?\n",
    "\n",
    "    # Transform training data\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    w = train_svm_linear(X_transformed, y,batch_size, C=C, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return w, omega, b\n",
    "\n",
    "def train_svm_adagrad_rff(X, y,batch_size=10, gamma= 0.01, n_features=500, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a SVM with RFF approximation for a Gaussian kernel.\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    - gamma: parameter of Gaussian kernel\n",
    "    - n_features: number of random features\n",
    "    - C: regularization strength\n",
    "    - epochs: number of training epochs\n",
    "    - lr: learning rate\n",
    "    \"\"\"\n",
    "    # Create Random Fourier Features transformation parameters\n",
    "    omega = np.random.normal(scale=2*np.pi*gamma, size=(X.shape[1], n_features))\n",
    "    b = np.random.uniform(0, 2*np.pi, size=n_features)\n",
    "    # random funktions that approximate ?\n",
    "\n",
    "    # Transform training data\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    w = train_svm_adagrad(X_transformed, y,batch_size, C=C, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return w, omega, b\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfe3a980f972166",
   "metadata": {},
   "source": [
    "## Find best parameters\n",
    "usasge of the find best parameter using K fold and normal apporach\n",
    "for each dataset as it is the best\n",
    "\n",
    "This code takes care of finding the best parameter.\n",
    "In the lists different values can be added in order to have a search space that is fitting, then all combinations are iterated and the best one for each dataset are returned\n",
    "However please consider this can take forever, the rff for toydata large takes about 5 hours i think so either limit the amount of data or the amount of combinations that are tested\n",
    "However we think that is a pretty good way to find the real best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79eb269e058e58c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T01:51:37.726441Z",
     "start_time": "2024-05-05T20:51:59.753376Z"
    },
    "metadata": {}
   },
   "source": [
    "# Example usage\n",
    "datasets_tiny_large = {\n",
    "    'toydate_tiny': load_dataset(\"toydata_tiny.csv\"),\n",
    "    #'toydata_large': load_dataset(\"toydata_large.csv\")#,\n",
    "}\n",
    "\n",
    "model_types = ['linear', 'adagrad', 'rff']\n",
    "\n",
    "# Defining parameter values\n",
    "C_values = [0.1, 0.5, 1.0]\n",
    "lr_values = [0.001, 0.1, 1.0]\n",
    "batch_size_values = [10, 20, 50]\n",
    "gamma_values = [0.001, 0.1,0.5,0.8]\n",
    "n_features_values = [100, 500, 2000]\n",
    "epochs_values = [1, 5]  # Added multiple values for epochs\n",
    "\n",
    "# Creating parameter combinations for each model\n",
    "params = {\n",
    "    'linear': [\n",
    "        {'C': C, 'epochs': epochs, 'lr': lr, 'batch_size': bs} \n",
    "        for C, epochs, lr, bs in itertools.product(C_values, epochs_values, lr_values, batch_size_values)\n",
    "    ],\n",
    "    'adagrad': [\n",
    "        {'C': C, 'epochs': epochs, 'lr': lr, 'batch_size': bs} \n",
    "        for C, epochs, lr, bs in itertools.product(C_values, epochs_values, lr_values, batch_size_values)\n",
    "    ],\n",
    "    'rff': [\n",
    "        {'C': C, 'epochs': epochs, 'lr': lr, 'batch_size': bs, 'gamma': g, 'n_features': nf} \n",
    "        for C, epochs, lr, bs, g, nf in itertools.product(C_values, epochs_values, lr_values, batch_size_values, gamma_values, n_features_values)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "results = evaluate_models_k(datasets_tiny_large, model_types, params)\n",
    "print(results)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b8c193cd66616ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T11:03:12.249098Z",
     "start_time": "2024-05-06T11:02:58.327959Z"
    }
   },
   "source": [
    "# commented out because of runtime if you just start the notebook\n",
    "'''\n",
    "datasets = {\n",
    "    'imdb': load_imdb_dataset(1000)\n",
    "}\n",
    "\n",
    "model_types = ['linear', 'adagrad']#, 'rff']\n",
    "\n",
    "# Defining parameter values\n",
    "C_values = [0.1,0.5, 1.0]\n",
    "lr_values = [0.001, 0.01, 0.1, 1.0]\n",
    "batch_size_values = [10, 20, 50]\n",
    "gamma_values = [0.001, 0.01, 0.1,0.5,0.8]\n",
    "n_features_values = [100, 500, 1000, 2000]\n",
    "epochs_values = [1, 10, 20]  # Added multiple values for epochs\n",
    "\n",
    "# Creating parameter combinations for each model\n",
    "params = {\n",
    "    'linear': [\n",
    "        {'C': C, 'epochs': epochs, 'lr': lr, 'batch_size': bs} \n",
    "        for C, epochs, lr, bs in itertools.product(C_values, epochs_values, lr_values, batch_size_values)\n",
    "    ],\n",
    "    'adagrad': [\n",
    "        {'C': C, 'epochs': epochs, 'lr': lr, 'batch_size': bs} \n",
    "        for C, epochs, lr, bs in itertools.product(C_values, epochs_values, lr_values, batch_size_values)\n",
    "    ],\n",
    "    'rff': [\n",
    "        {'C': C, 'epochs': epochs, 'lr': lr, 'batch_size': bs, 'gamma': g, 'n_features': nf} \n",
    "        for C, epochs, lr, bs, g, nf in itertools.product(C_values, epochs_values, lr_values, batch_size_values, gamma_values, n_features_values)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "results = evaluate_models(datasets, model_types, params)\n",
    "print(results)\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee6301e",
   "metadata": {},
   "source": [
    "## Ploting functions\n",
    "some function for the plotting in the report later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4730e582e875ca76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T09:15:48.724672Z",
     "start_time": "2024-05-04T09:15:48.431467Z"
    }
   },
   "source": [
    "#-------\n",
    "#plotting stuff\n",
    "\n",
    "\n",
    "\n",
    "# with history for plotting\n",
    "def train_svm_linear_hist(X, y, batch_size=10, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a linear SVM using SGD and log the loss per epoch.\n",
    "    \"\"\"\n",
    "    w = np.random.randn(X.shape[1]) / np.sqrt(X.shape[1])  # Improved initialization\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "    loss_history = []  # Initialize the loss history list\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(len(y))  # Shuffle the data\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(start + batch_size, len(y))\n",
    "            X_batch = X[perm[start:end]]\n",
    "            y_batch = y[perm[start:end]]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            w -= lr * grad\n",
    "        \n",
    "        # Calculate and record the loss at the end of each epoch\n",
    "        current_loss = svm_loss(w, X, y, C)\n",
    "        loss_history.append(current_loss)\n",
    "        \n",
    "        # Optionally, decrease learning rate gradually\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr /= 2\n",
    "    \n",
    "    return w, loss_history\n",
    "\n",
    "# SVM With plotting part\n",
    "def train_svm_linear_errorplot(X, y, batch_size=10, C=1.0, epochs=100, lr=0.01):\n",
    "    w = np.random.randn(X.shape[1]) / np.sqrt(X.shape[1])  # Improved initialization\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "    training_errors = []  # List to store error at each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(len(y))  # Shuffle the data\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(start + batch_size, len(y))\n",
    "            X_batch = X[perm[start:end]]\n",
    "            y_batch = y[perm[start:end]]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            w -= lr * grad\n",
    "        # Calculate error for current epoch\n",
    "        current_error = svm_loss(w, X, y, C)\n",
    "        training_errors.append(current_error)\n",
    "        \n",
    "        # Optionally, decrease learning rate gradually\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr /= 2\n",
    "    \n",
    "    # Plot the training errors\n",
    "    plt.plot(training_errors, label='Training Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Training Error Convergence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# with history for plotting\n",
    "def train_svm_rff_hist(X, y, batch_size=10, gamma=0.01, n_features=500, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a SVM with RFF approximation for a Gaussian kernel and log loss per epoch.\n",
    "    \"\"\"\n",
    "    # Create Random Fourier Features transformation parameters\n",
    "    omega = np.random.normal(scale=2*np.pi*gamma, size=(X.shape[1], n_features))\n",
    "    b = np.random.uniform(0, 2*np.pi, size=n_features)\n",
    "\n",
    "    # Transform training data\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    w, loss_history = train_svm_linear_hist(X_transformed, y, batch_size, C=C, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return w, omega, b, loss_history\n",
    "# with history for plotting\n",
    "def train_svm_adagrad_rff_hist(X, y, batch_size=10, gamma=0.01, n_features=500, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a SVM with RFF approximation for a Gaussian kernel and log loss per epoch.\n",
    "    \"\"\"\n",
    "    # Create Random Fourier Features transformation parameters\n",
    "    omega = np.random.normal(scale=2*np.pi*gamma, size=(X.shape[1], n_features))\n",
    "    b = np.random.uniform(0, 2*np.pi, size=n_features)\n",
    "\n",
    "    # Transform training data\n",
    "    X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "    w, loss_history = train_svm_adagrad_history(X_transformed, y, batch_size, C=C, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return w, omega, b, loss_history\n",
    "\n",
    "def plot_loss(sgd_losses, adagrad_losses, epochs):\n",
    "    \"\"\"\n",
    "    Plot the training error over epochs for SGD and Adagrad.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(epochs), sgd_losses, label='SGD Loss')\n",
    "    plt.plot(range(epochs), adagrad_losses, label='Adagrad Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Error over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # with history for plotting \n",
    "def train_svm_adagrad_history(X, y, batch_size=10, C=1.0, epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a linear SVM using AdaGrad optimization and log the loss per epoch.\n",
    "    \"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    epsilon = 1e-8  # smoothing term to avoid division by zero\n",
    "    gradient_accumulate = np.zeros(X.shape[1])  # saves the past gradients squared\n",
    "    loss_history = []  # Initialize the loss history list\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(len(y))  # Shuffle the data\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(start + batch_size, len(y))\n",
    "            X_batch = X[perm[start:end]]\n",
    "            y_batch = y[perm[start:end]]\n",
    "            grad = svm_gradient(w, X_batch, y_batch, C)\n",
    "            gradient_accumulate += grad ** 2\n",
    "            adjusted_lr = lr / (epsilon + np.sqrt(gradient_accumulate))  # Adjust learning rate depending on gradient magnitude\n",
    "            w -= adjusted_lr * grad\n",
    "        \n",
    "        # Calculate and record the loss at the end of each epoch\n",
    "        current_loss = svm_loss(w, X, y, C)\n",
    "        loss_history.append(current_loss)\n",
    "\n",
    "    return w, loss_history\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5e5769f5bd0e80",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "### Timings\n",
    "\n",
    "As asked in the report here are the timings for each of the parameters we choose as optimal for each of the datasets\n",
    "\n",
    "(we did not include the tiny dataset here as the time to compute it is really fast and therefore we think it is more important to look at these two examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad920757be422c",
   "metadata": {},
   "source": [
    "### **`toydata_large`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449f7166f81a1cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T10:31:23.812650Z",
     "start_time": "2024-05-06T10:31:19.793310Z"
    }
   },
   "source": [
    "# Timing and training with linear SVM\n",
    "start_time = time.time()\n",
    "train_svm_linear(X_large, y_large, 10, 0.5, 1, 0.1)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken by linear SVM method: {end_time - start_time} seconds\")\n",
    "\n",
    "# Timing and training with Adagrad\n",
    "start_time = time.time()\n",
    "train_svm_adagrad(X_large, y_large, 10, 1, 1, 0.1)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken by Adagrad method: {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "# Timing and training with Random Fourier Features and linear SVM\n",
    "start_time = time.time()\n",
    "train_svm_rff(X_large, y_large, 10, 0.1, 500,0.1, 1, 0.001)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for Random Fourier Features with linear SVM method: {end_time - start_time} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bc85053314799413",
   "metadata": {},
   "source": [
    "**`imdb`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103e4fef8b3e1d5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T10:26:13.253011Z",
     "start_time": "2024-05-06T10:26:13.021762Z"
    }
   },
   "source": [
    "# Timing and training with linear SVM\n",
    "start_time = time.time()\n",
    "train_svm_linear(X_imdb, y_imdb, 20, 1, 20, 0.001)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken by linear SVM method: {end_time - start_time} seconds\")\n",
    "\n",
    "# Timing and training with Adagrad\n",
    "start_time = time.time()\n",
    "train_svm_adagrad(X_imdb, y_imdb, 10, 0.1, 1, 0.001)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken by Adagrad method: {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "# Timing and training with Random Fourier Features and linear SVM\n",
    "start_time = time.time()\n",
    "train_svm_rff(X_imdb, y_imdb, 10, 0.1, 2000,10, 10, 0.001)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for Random Fourier Features with linear SVM method: {end_time - start_time} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6eed5745ff12644b",
   "metadata": {},
   "source": [
    "### Selection of Learning Rate and Regularization Parameter\n",
    "\n",
    "#### Hyperparameter Grids and Selection Criteria\n",
    "\n",
    "The selection of the best hyperparameters learning rate ($\\eta$), regularization parameter ($C$), and othersâ€”was based on systematic grid searches across different datasets.    \n",
    "Grid searches were performed separately for three datasets: `toydata_tiny`, `toydata_large`, and `imdb`.    \n",
    "Each dataset utilized a tailored grid of hyperparameters to optimize performance efficiently, considering computational resources and the complexity of the data.  \n",
    "\n",
    "#### Grid Specifications\n",
    "\n",
    "- **`toydata_tiny` and `imdb` datasets:**\n",
    "  - **Regularization parameter ($C$) values:** 0.1, 0.5, 1.0\n",
    "  - **Learning rate ($\\eta$) values:** 0.001, 0.01, 0.1, 1.0\n",
    "  - **Batch sizes:** 10, 20, 50\n",
    "  - **Number of epochs:** 1, 10, 20\n",
    "  - **Gamma ($\\gamma$) values for the RFF kernel approximation:** 0.001, 0.01, 0.1, 0.5, 0.8\n",
    "  - **Number of Fourier features:** 100, 500, 1000, 2000\n",
    "\n",
    "- **`toydata_large` dataset:**\n",
    "  - **Regularization parameter ($C$) values:** 0.1, 0.5, 1.0\n",
    "  - **Learning rate ($\\eta$) values:** 0.001, 0.1, 1.0\n",
    "  - **Batch sizes:** 10, 20, 50\n",
    "  - **Gamma ($\\gamma$) values:** 0.001, 0.1, 0.5, 0.8\n",
    "  - **Number of Fourier features:** 100, 500, 2000\n",
    "  - **Number of epochs:** 1, 5\n",
    "\n",
    "\n",
    "\n",
    "#### Selected Hyperparameters and Results\n",
    "\n",
    "The best hyperparameters were determined based on achieving the highest accuracy in classification tasks for each dataset and model type (linear, adagrad, rff). Below are the selected parameters for each dataset and model type:\n",
    "\n",
    "- **`toydata_large`:**\n",
    "  - **Linear model:** Best Parameters: $C = 0.5$, $ \\eta = 0.1$, batch size = 10, epochs = 1; Best Accuracy = 1.0\n",
    "  - **Adagrad model:** Best Parameters: $C = 1.0$, $ \\eta = 0.1$, batch size = 10, epochs = 1; Best Accuracy = 1.0\n",
    "  - **RFF model:** Best Parameters: $C = 0.1$, $ \\eta = 0.001$, batch size = 10, $ \\gamma = 0.1$, number of features = 500, epochs = 1; Best Accuracy = 1.0\n",
    "\n",
    "- **`toydata_tiny`:**\n",
    "  - **Linear model:** Best Parameters: $C = 10.0$, $ \\eta = 0.1$, batch size = 10, epochs = 1; Best Accuracy = 0.995\n",
    "  - **Adagrad model:** Best Parameters: $C = 10.0$, $ \\eta = 0.5$, batch size = 10, epochs = 1; Best Accuracy = 0.995\n",
    "  - **RFF model:** Best Parameters: $C = 0.1$, $ \\eta = 0.1$, batch size = 20, $ \\gamma = 0.1$, number of features = 1000, epochs = 20; Best Accuracy = 1.0\n",
    "\n",
    "- **`imdb`:**\n",
    "  - **Linear model:** Best Parameters: $C = 0.1$, $ \\eta = 0.001$, batch size = 50, epochs = 20; Best Accuracy = 0.763\n",
    "  - **Adagrad model:** Best Parameters: $C = 0.1$, $ \\eta = 0.001$, batch size = 20, epochs = 20; Best Accuracy = 0.765\n",
    "  - **RFF model:** Best Parameters: $C = 10.0$, $ \\eta = 0.001$, batch size = 10, $ \\gamma = 0.1$, number of features = 2000, epochs = 10; Best Accuracy = 1.0\n",
    "\n",
    "#### Model Performance Summary\n",
    "\n",
    "The computational times for different methods on each dataset were as follows:\n",
    "\n",
    "##### `toydata_large` dataset:\n",
    "- **Linear SVM method:** 0.933 seconds\n",
    "- **Adagrad method:** 0.588 seconds\n",
    "- **Random Fourier Features with linear SVM method:** 2.489 seconds\n",
    "\n",
    "##### `imdb` dataset:\n",
    "- **Linear SVM method:** 0.072 seconds\n",
    "- **Adagrad method:** 0.011 seconds\n",
    "- **Random Fourier Features with linear SVM method:** 0.143 seconds\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "Each dataset required adjustments to the hyperparameters due to differences in data complexity and size. The large dataset (`toydata_large`) required a narrower range of epochs and a smaller set of learning rate options due to computational constraints, despite its successful outcomes. The different models within the same dataset also showed variability in optimal parameters, emphasizing the need for tailored approaches based on both the model type and the specific characteristics of the dataset.\n",
    "\n",
    "However some similarities can be seen for example the batch size of 10 seems to be the best one in most cases. Also the learning rate should be kind of small meaning that the learning rates of 0.5+ were never selected while we tested some of them. Also interesting is that the gamma was most of the times choosen with 0.1 and the number of features is not always the biggest one that was choosen for the rff. All of this can of course depend on some randomness that comes with each test. \n",
    "\n",
    "Obvious is also from the optimal parameters that they match much closer between the both toydatas than between the toydata and the imdb as these two datasets are much more similar. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548ac0367db6adf",
   "metadata": {},
   "source": [
    "\n",
    "## SVM\n",
    "### Include a plot illustrating convergence of SGD for your selected hyperparamters (this can be for a single fold, and should show the training error over the number of SGD epochs).\n",
    "\n",
    "Shown below for each of the datasets, adjusted epochs for the toydata large as this one takes the longest, if you wish you can also go with 100 epochs here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45be14ec2ed4c99",
   "metadata": {},
   "source": [
    "**`toydata_tiny`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23725f30ee1959d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T10:06:47.193180Z",
     "start_time": "2024-05-06T10:06:46.972304Z"
    }
   },
   "source": [
    "train_svm_linear_errorplot(X_tiny,y_tiny, 10,10, 100, 0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91242f37c004562d",
   "metadata": {},
   "source": [
    "### **`toydata_large`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d95f998a6f4b7932",
   "metadata": {},
   "source": [
    "train_svm_linear_errorplot(X_large,y_large, 10, 0.5, 50, 0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3b9b2a2828b5dda4",
   "metadata": {},
   "source": [
    "**`imdb`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddb5175f35ffa8da",
   "metadata": {},
   "source": [
    "train_svm_linear_errorplot(X_imdb,y_imdb, 20, 1, 100, 0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c3426d9021ef1868",
   "metadata": {},
   "source": [
    "### Briefly discuss your implementation of the SVM and SGD.\n",
    "svm_loss function:\n",
    "\n",
    "Calculates the SVM loss, which is the sum of the hinge loss and the L2 regularization loss.\n",
    "The hinge loss is computed as: C * np.mean(np.maximum(0, 1 - y * np.dot(X, W))), where C is the regularization strength, y is the target vector, X is the feature matrix, and W is the weight vector.\n",
    "The regularization loss is 0.5 * np.dot(W, W).\n",
    "\n",
    "\n",
    "svm_gradient function:\n",
    "\n",
    "Calculates the gradient of the SVM loss with respect to the weight vector W.\n",
    "Iterates over the batch and computes the gradient contribution for each misclassified point: C * (-y_i * x_i), where y_i is the target value and x_i is the feature vector for the i-th data point.\n",
    "Averages the gradient over the batch size.\n",
    "Adds the gradient of the regularization term (W) to the gradient.\n",
    "\n",
    "\n",
    "train_svm_linear function:\n",
    "\n",
    "Initializes the weight vector w with random values.\n",
    "Shuffles the data and divides it into batches.\n",
    "For each batch:\n",
    "\n",
    "Calculates the gradient using svm_gradient.\n",
    "Updates the weight vector w using SGD: w -= lr * grad, where lr is the learning rate.\n",
    "\n",
    "\n",
    "Optionally decreases the learning rate gradually.\n",
    "Returns the final weight vector w after all epochs.\n",
    "\n",
    "\n",
    "\n",
    "Our implementation utilizes SGD to optimize the SVM loss function, which combines the hinge loss for misclassified data points and L2 regularization to prevent overfitting. The gradient of the loss is calculated for each batch, and the weights are updated using SGD, iterating over multiple epochs to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3581fb05b4c0bb",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "### What is the impact of the adagrad regarding the performance in comparison to standard SGD for the different datasets?\n",
    "\n",
    "- **`toydata_large` dataset:**\n",
    "  - **SGD (Linear model):** Best Accuracy = 1.0\n",
    "  - **Adagrad (Linear model):** Best Accuracy = 1.0\n",
    "  - *Observation:* Both methods performed identically in terms of accuracy. \n",
    "\n",
    "- **`toydata_tiny` dataset:**\n",
    "  - **SGD (Linear model):** Best Accuracy = 0.995\n",
    "  - **Adagrad (Linear model):** Best Accuracy = 0.995\n",
    "  - *Observation:* As with the `toydata_large` dataset, no difference in final accuracy was noted. \n",
    "\n",
    "- **`imdb` dataset:**\n",
    "  - **SGD (Linear model):** Best Accuracy = 0.763\n",
    "  - **Adagrad (Linear model):** Best Accuracy = 0.765\n",
    "  - *Observation:* The Adagrad optimizer showed a slight improvement over standard SGD. This suggests that Adagrad's adaptive learning rates were beneficial for handling the more diverse and complex features.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Impact of Adagrad compared to standard SGD varies based on the complexity and characteristics of the dataset:\n",
    "- When the feature scales are relatively uniform and the data is less sparse (`toydata` series), the advantage of Adagrad in terms of performance might not be significant.\n",
    "- Also the first dataset is really easy for a svm and therefore all implementations perform really well and there is not too much difference to be seen here \n",
    "- For datasets like `imdb`, which likely contain more sparse and irregular data (common in text data), Adagrad can outperform standard SGD by adapting the learning rates to the nature of each feature, leading to more efficient learning dynamics.\n",
    "\n",
    "### Repeat your experiments using random dropout of input features with probabilityp = 0.5 for the IMDB data. Does this change affect performance and if so, how? \n",
    "\n",
    "So first I thought about what should happen. As the random dropout decreases the amount of features the algorithm can learn from it should decrease the performance (accuracy) of the algorithm. I mean it is probably faster but the accuracy should decrease. \n",
    "\n",
    "Results with droput:\n",
    "{'imdb': {'adagrad': {'Best Parameters': {'C': 0.5, 'epochs': 20, 'lr': 0.001, 'batch_size': 20}, 'Best Accuracy': 0.724}}}\n",
    "Runtime 6 seconds\n",
    "\n",
    "Results without dropout:\n",
    "{'imdb': {'adagrad': {'Best Parameters': {'C': 0.5, 'epochs': 20, 'lr': 0.001, 'batch_size': 20}, 'Best Accuracy': 0.74}}}\n",
    "Runtime 7 seconds\n",
    "\n",
    "So the accuracy decreased by 0.016 but the runtime decreased by 1 second. So the algorithm is faster but less accurate. Like predicted.\n",
    "However I am pretty suprised that the accuracy is still pretty high. I would have expected a bigger decrease in accuracy, as many \n",
    "\n",
    "\n",
    "### Briefly discuss your implementation of Adagrad. How did you adjust it to perform stochastic optimization instead of online learning. Provide pseudo-code.\n",
    "The code is adapted in order to handle the mini batches instead of the online learning. \n",
    "At the beginning the vector w is inialized to zero and the gradient accumulate is also set to zero. \n",
    "Then the number of batches is calculated and the data is shuffled.\n",
    "\n",
    "For each batch the gradient is calculated and the gradient accumulate is updated. The learning rate is adjusted by dividing the learning rate by the square root of the gradient accumulate. The weights are updated by subtracting the adjusted learning rate times the gradient.\n",
    "\n",
    "the gradient gets updated in each iteration by updating the gradient the adjusted learning rate is calculated \n",
    "Each parameter has its own learning rate which is adjusted by the gradient accumulate. This is for the correct implementation of the adagrad algorithm.\n",
    "\n",
    "peseudo code implementation (however in my opinion just looking at the code is more usefull)\n",
    "'''\n",
    "Initialize weights vector 'w' as zero\n",
    "Initialize 'gradient_accumulate' as zero\n",
    "Set 'epsilon' to a small constant (e.g., 1e-8) to avoid division by zero\n",
    "\n",
    "For each epoch from 1 to total_epochs:\n",
    "    For each batch from 1 to total_number_of_batches:\n",
    "        Extract batch 'X_batch' and 'y_batch' from 'X' and 'y'\n",
    "        \n",
    "        Calculate the gradient 'grad' of the loss function with respect to 'w' using 'X_batch' and 'y_batch'\n",
    "        Square 'grad' and add to 'gradient_accumulate'\n",
    "        \n",
    "        Adjust learning rate for each weight:\n",
    "            'adjusted_lr' = initial_learning_rate / (sqrt('gradient_accumulate') + 'epsilon')\n",
    "        \n",
    "        Update weights 'w':\n",
    "            'w' = 'w' - 'adjusted_lr' * 'grad'\n",
    "\n",
    "Return 'w'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456388938fdb84a",
   "metadata": {},
   "source": [
    "## RFF\n",
    "\n",
    "### Include a plot illustrating convergence of SGD/Adagrad for your selected hyper paramters (this can be for a single fold, and should show the training error over the number of SGD/Adagrad epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f888c9abb372c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T11:46:21.494193Z",
     "start_time": "2024-05-06T11:46:20.074740Z"
    }
   },
   "source": [
    "# Training the models\n",
    "_, _, _, sgd_losses = train_svm_rff_hist(X_imdb, y_imdb, batch_size=10, gamma=0.01, n_features=500, C=1.0, epochs=50, lr=0.01)\n",
    "_, _, _, adagrad_losses = train_svm_adagrad_rff_hist(X_imdb, y_imdb, batch_size=10, gamma=0.01, n_features=500, C=1.0, epochs=50, lr=0.01)\n",
    "\n",
    "# Plotting the losses\n",
    "plot_loss(sgd_losses, adagrad_losses, epochs=50)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "994357373e47b150",
   "metadata": {},
   "source": [
    "\n",
    "### Does the number of RFFs affect the classification accuracy? If so, how?\n",
    "The number of RFFs directly affects the model's ability to capture complex patterns in the data:\n",
    "\n",
    "- **Dimensionality vs. Complexity:** Increasing the number of RFFs enhances the model's capacity to represent more complex functions. A higher dimensionality can improve the accuracy of the approximation to the desired kernel function, potentially leading to better classification performance.\n",
    "\n",
    "- **Overfitting vs. Underfitting:** With too few Fourier features, the model may underfit, failing to capture essential data characteristics. Conversely, too many features might lead to overfitting, particularly in scenarios with limited training data, where the model learns noise in the training set rather than generalizing from true underlying patterns.\n",
    "\n",
    "- **Computational Efficiency:** A larger number of features increases computational cost, both in terms of memory and processing time. This factor must be balanced against the potential gains in model accuracy and generalization.\n",
    "\n",
    "- **Toydata Tiny and Large datasets:** A moderate number of RFFs (e.g., 500-1000) already achieved the maximum accuracy. This indicates that beyond a certain threshold, additional features do not necessarily improve performance, likely due to the simpler nature of the datasets.\n",
    "\n",
    "- **IMDb dataset:** A significant improvement in accuracy was observed when increasing the number of RFFs from 1000 to 2000. This improvement suggests that for more complex datasets with intricate patterns, a higher number of features can better capture the complexity and nuances of the data.\n",
    "\n",
    "### Briefly discuss your implementation of RFFs.\n",
    "$\\omega$ is sampled from the Gaussian distribution $N(0, 2\\pi \\gamma)$ and $b$ is sampled uniformly from $[0, 2\\pi]$.  \n",
    "\n",
    "```python\n",
    "omega = np.random.normal(scale=2*np.pi*gamma, size=(X.shape[1], n_features))\n",
    "b = np.random.uniform(0, 2*np.pi, size=n_features)\n",
    "```\n",
    "   \n",
    "Then the input data X is transformed using the approximate feature map $\\phi(x) = \\cos(X \\cdot \\omega + b)$.   \n",
    "\n",
    "```python\n",
    "X_transformed = rff_gaussian_transform(X, omega, b)\n",
    "def rff_gaussian_transform(X, omega, b):\n",
    "X_features = np.dot(X, omega) + b\n",
    "return np.cos(X_features)\n",
    "```\n",
    "   \n",
    "This transformed feature set is an approximation of the high-dimensional space of the Gaussian kernel.\n",
    "Now we can use the transformed features in the SVM, as if it were using the original kernel.   \n",
    " \n",
    "```python\n",
    "w = train_svm_linear(X_transformed, y, batch_size, C=C, epochs=epochs, lr=lr)\n",
    "```\n",
    "\n",
    "The other versio nof our RFF implementation useses the adagrad svm instead of this regular svm. The results between the two can be found in the graph above.\n",
    "The code for the adagrad version just changes one line basically\n",
    "\n",
    "```python\n",
    "w = train_svm_adagrad(X_transformed, y, batch_size, C=C, epochs=epochs, lr=lr)\n",
    "```\n",
    "\n",
    "\n",
    "### For IMDB, report the runtime and performance (in plots or a single plot) when training on 1000, 2000, and 3000 training samples, respectively. Report the same when using sklearnâ€™s svm.SVC class. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf3f4c16f5e1b787",
   "metadata": {},
   "source": [
    "# Timing and training with Random Fourier Features and linear SVM\n",
    "start_time = time.time()\n",
    "train_svm_rff(X_imdb, y_imdb, 10, 0.1, 2000,10, 10, 0.001)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for Random Fourier Features with linear SVM method: {end_time - start_time} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7862982b365f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:09:05.129064Z",
     "start_time": "2024-05-06T12:09:02.579379Z"
    }
   },
   "source": [
    "sample_sizes = [1000, 2000, 3000]\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = []\n",
    "\n",
    "for size in sample_sizes:\n",
    "    X_subset = X_imdb[:size]\n",
    "    y_subset = y_imdb[:size]\n",
    "    \n",
    "    # Custom SVM with RFF\n",
    "    custom_time_start = time.time()\n",
    "    w, omega, b = train_svm_rff(X_subset, y_subset, batch_size=10, gamma=0.1, n_features=2000, C=10, epochs=50, lr=0.001)\n",
    "    predictions = predict_rff(X_subset, w, omega, b)\n",
    "    custom_accuracy = accuracy(y_subset, predictions)  \n",
    "    print(custom_accuracy)\n",
    "    custom_time = time.time() - custom_time_start\n",
    "\n",
    "    # sklearn SVM\n",
    "    clf = SVC(kernel='rbf', gamma='scale')\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_subset, y_subset)\n",
    "    sklearn_time = time.time() - start_time\n",
    "    sklearn_accuracy = clf.score(X_subset, y_subset)\n",
    "\n",
    "    results.append((custom_time, custom_accuracy, sklearn_time, sklearn_accuracy))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe3ca48d20d5074",
   "metadata": {},
   "source": [
    "# Extract information\n",
    "custom_times, custom_accuracies, sklearn_times, sklearn_accuracies = zip(*results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Runtime comparison\n",
    "ax1.plot(sample_sizes, custom_times, 'o-', color='red', label='Custom SVM with RFF')\n",
    "ax1.plot(sample_sizes, sklearn_times, 'x--', color='blue', label='Sklearn SVM')\n",
    "ax1.set_title('Runtime Comparison')\n",
    "ax1.set_xlabel('Number of Training Samples')\n",
    "ax1.set_ylabel('Runtime (seconds)')\n",
    "ax1.legend()\n",
    "ax1.grid(True)  # Adding grid for better readability\n",
    "\n",
    "\n",
    "# Accuracy comparison\n",
    "ax2.plot(sample_sizes, custom_accuracies, 'o-', color='red', label='Custom SVM with RFF')\n",
    "ax2.plot(sample_sizes, sklearn_accuracies, 'x--', color='blue', label='Sklearn SVM')\n",
    "ax2.set_title('Accuracy Comparison')\n",
    "ax2.set_xlabel('Number of Training Samples')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)  # Adding grid for better readability\n",
    "\n",
    "# Ensuring all plots are properly spaced\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "17903560",
   "metadata": {},
   "source": [
    "Here we see that it is very interesting that the runtime for our implementation is not always getting lower with more training samples (However if you rerun the code for the graph times this can change I tried it multiple times and the results really vary a lot sometimes, both for the sklearn svm as well as our implementation). But this can be the case if the functions converges faster because there are more infos to be used. Also mabye overfitting is reduced this way and the result better. However the results stay pretty much the same between all the different number of training samples as can be seen in the right plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
