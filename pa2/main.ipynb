{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta_0=1, lambda_reg=0.01, accuracy=1.0\n",
      "eta_0=10, lambda_reg=1, accuracy=0.99969375\n",
      "eta_0=100, lambda_reg=10, accuracy=0.50003125\n",
      "eta_0=1000, lambda_reg=100, accuracy=0.49999375\n",
      "eta_0=0.1, lambda_reg=0.0, accuracy=1.0\n",
      "eta_0=0.001, lambda_reg=0.01, accuracy=0.99999375\n",
      "eta_0=0.001, lambda_reg=0.001, accuracy=0.99999375\n",
      "eta_0=0.0001, lambda_reg=0.01, accuracy=0.99998125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Test each set of parameters\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eta_0, lambda_reg \u001b[38;5;129;01min\u001b[39;00m parameter_sets:\n\u001b[1;32m---> 93\u001b[0m     accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtest_online_svm_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     global_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(accuracies)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(accuracies)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# print results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mtest_online_svm_accuracy\u001b[1;34m(X, y, eta_0, lambda_reg, epochs, test_size)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_online_svm_accuracy\u001b[39m(X, y, eta_0, lambda_reg, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[0;32m     57\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m     final_weights, accuracies \u001b[38;5;241m=\u001b[39m \u001b[43monline_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracies\n",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m, in \u001b[0;36monline_svm\u001b[1;34m(X_train, y_train, X_test, y_test, eta_0, lambda_reg, epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m eta_t \u001b[38;5;241m=\u001b[39m eta_0 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     43\u001b[0m w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m eta_t \u001b[38;5;241m*\u001b[39m hinge_loss_gradient(w, x_t, y_t, lambda_reg)\n\u001b[1;32m---> 44\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# calculate class of x_t\u001b[39;00m\n\u001b[0;32m     46\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msign(np\u001b[38;5;241m.\u001b[39mdot(w, x_t))\n",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m, in \u001b[0;36mproject\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject\u001b[39m(w):\n\u001b[1;32m---> 29\u001b[0m     norm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     31\u001b[0m         w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m/\u001b[39m norm\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\linalg\\linalg.py:2552\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2550\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2552\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2553\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset_small(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Function to compute hinge loss with L2 regularization\n",
    "def hinge_loss(w, x, y, lambda_reg):\n",
    "    return max(0, 1 - y * np.dot(w, x)) + (lambda_reg / 2) * np.linalg.norm(w)**2\n",
    "\n",
    "# Function to compute the gradient of hinge loss with L2 regularization\n",
    "def hinge_loss_gradient(w, x, y, lambda_reg):\n",
    "    if y * np.dot(w, x) < 1:\n",
    "        return -y * x + lambda_reg * w\n",
    "    else:\n",
    "        return lambda_reg * w\n",
    "\n",
    "# Optional projection to limit the norm of w\n",
    "def project(w):\n",
    "    norm = np.linalg.norm(w)\n",
    "    if norm > 1:\n",
    "        w = w / norm\n",
    "    return w\n",
    "\n",
    "# Online SVM with regularization\n",
    "def online_svm(X_train, y_train, X_test, y_test, eta_0, lambda_reg, epochs=1):\n",
    "    n_features = X_train.shape[1]\n",
    "    w = np.zeros(n_features)\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for t, (x_t, y_t) in enumerate(zip(X_train, y_train)):\n",
    "            eta_t = eta_0 * (1 / np.sqrt(t + 1))\n",
    "            w -= eta_t * hinge_loss_gradient(w, x_t, y_t, lambda_reg)\n",
    "            w = project(w)\n",
    "            # calculate class of x_t\n",
    "            y_pred = np.sign(np.dot(w, x_t))\n",
    "            # calculate accuracy\n",
    "            if y_pred == y_t:\n",
    "                accuracies.append(1)\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "\n",
    "    return w, accuracies\n",
    "\n",
    "# Function to test the model accuracy\n",
    "def test_online_svm_accuracy(X, y, eta_0, lambda_reg, epochs=1, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    final_weights, accuracies = online_svm(X_train, y_train, X_test, y_test, eta_0, lambda_reg, epochs)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define different parameter sets for eta_0 and lambda_reg\n",
    "parameter_sets = [\n",
    "    (1,0.01),\n",
    "    (10,1),\n",
    "    (100,10),\n",
    "    (1000,100),\n",
    "    (0.1,0.0),\n",
    "    (1e-3, 1e-2),\n",
    "    (1e-3, 1e-3),\n",
    "    (1e-4, 1e-2),\n",
    "    (1e-4, 1e-3),\n",
    "    (1e-4, 1e-4),\n",
    "    (1e-5, 1e-2),\n",
    "    (1e-5, 1e-3),\n",
    "    (1e-5, 1e-4),\n",
    "    (1e-6, 1e-2),\n",
    "    (1e-6, 1e-3),\n",
    "    (1e-6, 1e-4)\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_dataset_small(\"toydata_large.csv\")  # Update the file path accordingly\n",
    "\n",
    "# Store results for plotting\n",
    "results = []\n",
    "\n",
    "# Test each set of parameters\n",
    "for eta_0, lambda_reg in parameter_sets:\n",
    "    accuracies = test_online_svm_accuracy(X, y, eta_0, lambda_reg, epochs=1)\n",
    "    global_accuracy = sum(accuracies)/len(accuracies)\n",
    "    # print results\n",
    "    print(f\"eta_0={eta_0}, lambda_reg={lambda_reg}, accuracy={global_accuracy}\")\n",
    "    \n",
    "    results.append((eta_0, lambda_reg, global_accuracy))\n",
    "\n",
    "# Function to train and evaluate an SVM model\n",
    "def train_and_evaluate_svm_python(X, y, kernel='linear', C=1.0):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create the SVM classifier\n",
    "    svm_classifier = SVC(kernel=kernel, C=C)\n",
    "\n",
    "    # Train the SVM classifier\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = train_and_evaluate_svm_python(X, y)\n",
    "print(f\"Accuracy of the SVM model: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'C': 0.01, 'eta': 0.1, 'epochs': 2, 'batch_size': 10}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 0.1, 'eta': 0.01, 'epochs': 5, 'batch_size': 500}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 0.5, 'eta': 0.005, 'epochs': 2, 'batch_size': 200}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 1.0, 'eta': 0.001, 'epochs': 7, 'batch_size': 50}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 2.0, 'eta': 0.0005, 'epochs': 2, 'batch_size': 100}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 0.05, 'eta': 0.05, 'epochs': 3, 'batch_size': 30}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 3.0, 'eta': 0.002, 'epochs': 5, 'batch_size': 75}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 5.0, 'eta': 0.001, 'epochs': 1, 'batch_size': 250}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 10.0, 'eta': 0.0001, 'epochs': 3, 'batch_size': 400}\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 0.2, 'eta': 0.01, 'epochs': 5, 'batch_size': 25}\n",
      "Accuracy: 95.00%\n",
      "Parameters: {'C': 0.01, 'eta': 0.1, 'epochs': 2, 'batch_size': 10}, Accuracy: 95.00%\n",
      "Parameters: {'C': 0.1, 'eta': 0.01, 'epochs': 5, 'batch_size': 500}, Accuracy: 95.00%\n",
      "Parameters: {'C': 0.5, 'eta': 0.005, 'epochs': 2, 'batch_size': 200}, Accuracy: 95.00%\n",
      "Parameters: {'C': 1.0, 'eta': 0.001, 'epochs': 7, 'batch_size': 50}, Accuracy: 95.00%\n",
      "Parameters: {'C': 2.0, 'eta': 0.0005, 'epochs': 2, 'batch_size': 100}, Accuracy: 95.00%\n",
      "Parameters: {'C': 0.05, 'eta': 0.05, 'epochs': 3, 'batch_size': 30}, Accuracy: 95.00%\n",
      "Parameters: {'C': 3.0, 'eta': 0.002, 'epochs': 5, 'batch_size': 75}, Accuracy: 95.00%\n",
      "Parameters: {'C': 5.0, 'eta': 0.001, 'epochs': 1, 'batch_size': 250}, Accuracy: 95.00%\n",
      "Parameters: {'C': 10.0, 'eta': 0.0001, 'epochs': 3, 'batch_size': 400}, Accuracy: 95.00%\n",
      "Parameters: {'C': 0.2, 'eta': 0.01, 'epochs': 5, 'batch_size': 25}, Accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# ohne adagrad glaub ich\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    # Manual standardization\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X = (X - mean) / std\n",
    "    return X, y\n",
    "\n",
    "def hinge_loss(w, X, y, C):\n",
    "    # Calculate hinge loss and gradient\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    distances[distances < 0] = 0  # max(0, 1-y*(w.x))\n",
    "    hinge_loss = C * np.mean(distances) + 0.5 * np.sum(w * w)\n",
    "    return hinge_loss\n",
    "\n",
    "def hinge_loss_gradient(w, X, y, C):\n",
    "    # Calculate gradient of hinge loss\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    dw = np.zeros(len(w))\n",
    "    for ind, d in enumerate(distances):\n",
    "        if d > 0:\n",
    "            dw += -y[ind] * X[ind]\n",
    "    dw = dw / len(y)  # Average\n",
    "    dw = dw * C + w  # Add regularization term\n",
    "    return dw\n",
    "\n",
    "def train_svm(X, y, C, eta, epochs, batch_size):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    n_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            grad = hinge_loss_gradient(w, X_batch, y_batch, C)\n",
    "            # Update rule\n",
    "            w -= eta * grad\n",
    "        \n",
    "        # Logging progress\n",
    "        \n",
    "        #print(f\"Epoch {epoch + 1}, Loss: {hinge_loss(w, X, y, C)}\")\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Parameters\n",
    "file_path = 'toydata_tiny.csv'  # Replace with your actual file path\n",
    "C = 0.01  # Regularization parameter\n",
    "eta = 1.1  # Learning rate\n",
    "epochs = 5\n",
    "batch_size = 5\n",
    "\n",
    "# Load and train SVM\n",
    "X, y = load_dataset(file_path)\n",
    "# split into training and testing sets\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "w = train_svm(X, y, C, eta, epochs, batch_size)\n",
    "\n",
    "def predict(X, w):\n",
    "    # Calculate predictions based on sign of the dot product\n",
    "    predictions = np.dot(X, w)\n",
    "    return np.where(predictions >= 0, 1, -1)\n",
    "\n",
    "def evaluate_accuracy(X, y, w):\n",
    "    # Get predictions\n",
    "    predictions = predict(X, w)\n",
    "    # Compare predictions with actual labels\n",
    "    correct_predictions = np.sum(predictions == y)\n",
    "    total_predictions = len(y)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy * 100  # return as percentage\n",
    "\n",
    "# Define different sets of parameters to test\n",
    "parameters = [\n",
    "    {'C': 0.01, 'eta': 0.1, 'epochs': 2, 'batch_size': 10},\n",
    "    {'C': 0.1, 'eta': 0.01, 'epochs': 5, 'batch_size': 500},\n",
    "    {'C': 0.5, 'eta': 0.005, 'epochs': 2, 'batch_size': 200},\n",
    "    {'C': 1.0, 'eta': 0.001, 'epochs': 7, 'batch_size': 50},\n",
    "    {'C': 2.0, 'eta': 0.0005, 'epochs': 2, 'batch_size': 100},\n",
    "    {'C': 0.05, 'eta': 0.05, 'epochs': 3, 'batch_size': 30},\n",
    "    {'C': 3.0, 'eta': 0.002, 'epochs': 5, 'batch_size': 75},\n",
    "    {'C': 5.0, 'eta': 0.001, 'epochs': 1, 'batch_size': 250},\n",
    "    {'C': 10.0, 'eta': 0.0001, 'epochs': 3, 'batch_size': 400},\n",
    "    {'C': 0.2, 'eta': 0.01, 'epochs': 5, 'batch_size': 25}\n",
    "]\n",
    "\n",
    "# Test each parameter configuration\n",
    "results = []\n",
    "for params in parameters:\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "    w = train_svm(X, y, params['C'], params['eta'], params['epochs'], params['batch_size'])\n",
    "    accuracy = evaluate_accuracy(X_test, y_test, w)\n",
    "    results.append((params, accuracy))\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Display results\n",
    "for params, accuracy in results:\n",
    "    print(f\"Parameters: {params}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'C': 0.01, 'initial_eta': 0.1, 'epochs': 2, 'batch_size': 10}\n",
      "Epoch 1, Loss: 0.009945266744137577\n",
      "Epoch 2, Loss: 0.009945268060759002\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 0.1, 'initial_eta': 0.01, 'epochs': 5, 'batch_size': 500}\n",
      "Epoch 1, Loss: 0.09871624622692504\n",
      "Epoch 2, Loss: 0.09798666242188872\n",
      "Epoch 3, Loss: 0.09747829320008093\n",
      "Epoch 4, Loss: 0.09709303585674844\n",
      "Epoch 5, Loss: 0.09678696346255218\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 0.5, 'initial_eta': 0.005, 'epochs': 2, 'batch_size': 200}\n",
      "Epoch 1, Loss: 0.4965656123261285\n",
      "Epoch 2, Loss: 0.49418469871598886\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 1.0, 'initial_eta': 0.001, 'epochs': 7, 'batch_size': 50}\n",
      "Epoch 1, Loss: 0.9968122603757017\n",
      "Epoch 2, Loss: 0.9949493235007117\n",
      "Epoch 3, Loss: 0.9934916480413791\n",
      "Epoch 4, Loss: 0.9922540499159936\n",
      "Epoch 5, Loss: 0.9911601170489449\n",
      "Epoch 6, Loss: 0.9901694922531403\n",
      "Epoch 7, Loss: 0.9892577793432363\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 2.0, 'initial_eta': 0.0005, 'epochs': 2, 'batch_size': 100}\n",
      "Epoch 1, Loss: 1.9975771526098158\n",
      "Epoch 2, Loss: 1.9960840209789668\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 0.05, 'initial_eta': 0.05, 'epochs': 3, 'batch_size': 30}\n",
      "Epoch 1, Loss: 0.04863352291882861\n",
      "Epoch 2, Loss: 0.048632364163250286\n",
      "Epoch 3, Loss: 0.04863228065182689\n",
      "Accuracy: 95.00%\n",
      "Training with parameters: {'C': 3.0, 'initial_eta': 0.002, 'epochs': 5, 'batch_size': 75}\n",
      "Epoch 1, Loss: 2.985396264696098\n",
      "Epoch 2, Loss: 2.976553632226172\n",
      "Epoch 3, Loss: 2.9695623017068247\n",
      "Epoch 4, Loss: 2.963595737943539\n",
      "Epoch 5, Loss: 2.958304705744917\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 5.0, 'initial_eta': 0.001, 'epochs': 1, 'batch_size': 250}\n",
      "Epoch 1, Loss: 4.993082224324294\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 10.0, 'initial_eta': 0.0001, 'epochs': 3, 'batch_size': 400}\n",
      "Epoch 1, Loss: 9.998616254864352\n",
      "Epoch 2, Loss: 9.997637818437463\n",
      "Epoch 3, Loss: 9.99683893937467\n",
      "Accuracy: 85.00%\n",
      "Training with parameters: {'C': 0.2, 'initial_eta': 0.01, 'epochs': 5, 'batch_size': 25}\n",
      "Epoch 1, Loss: 0.1916116510879348\n",
      "Epoch 2, Loss: 0.18842909290890747\n",
      "Epoch 3, Loss: 0.1864183606483709\n",
      "Epoch 4, Loss: 0.1849827983034754\n",
      "Epoch 5, Loss: 0.18389250527763726\n",
      "Accuracy: 85.00%\n",
      "Parameters: {'C': 0.01, 'initial_eta': 0.1, 'epochs': 2, 'batch_size': 10}, Accuracy: 95.00%\n",
      "Parameters: {'C': 0.1, 'initial_eta': 0.01, 'epochs': 5, 'batch_size': 500}, Accuracy: 85.00%\n",
      "Parameters: {'C': 0.5, 'initial_eta': 0.005, 'epochs': 2, 'batch_size': 200}, Accuracy: 85.00%\n",
      "Parameters: {'C': 1.0, 'initial_eta': 0.001, 'epochs': 7, 'batch_size': 50}, Accuracy: 85.00%\n",
      "Parameters: {'C': 2.0, 'initial_eta': 0.0005, 'epochs': 2, 'batch_size': 100}, Accuracy: 85.00%\n",
      "Parameters: {'C': 0.05, 'initial_eta': 0.05, 'epochs': 3, 'batch_size': 30}, Accuracy: 95.00%\n",
      "Parameters: {'C': 3.0, 'initial_eta': 0.002, 'epochs': 5, 'batch_size': 75}, Accuracy: 85.00%\n",
      "Parameters: {'C': 5.0, 'initial_eta': 0.001, 'epochs': 1, 'batch_size': 250}, Accuracy: 85.00%\n",
      "Parameters: {'C': 10.0, 'initial_eta': 0.0001, 'epochs': 3, 'batch_size': 400}, Accuracy: 85.00%\n",
      "Parameters: {'C': 0.2, 'initial_eta': 0.01, 'epochs': 5, 'batch_size': 25}, Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "# adagard version \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    # Manual standardization\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X = (X - mean) / std\n",
    "    return X, y\n",
    "\n",
    "def hinge_loss(w, X, y, C):\n",
    "    # Calculate hinge loss and gradient\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    distances[distances < 0] = 0  # max(0, 1-y*(w.x))\n",
    "    hinge_loss = C * np.mean(distances) + 0.5 * np.sum(w * w)\n",
    "    return hinge_loss\n",
    "\n",
    "def hinge_loss_gradient(w, X, y, C):\n",
    "    # Calculate gradient of hinge loss\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    dw = np.zeros(len(w))\n",
    "    for ind, d in enumerate(distances):\n",
    "        if d > 0:\n",
    "            dw += -y[ind] * X[ind]\n",
    "    dw = dw / len(y)  # Average\n",
    "    dw = dw * C + w  # Add regularization term\n",
    "    return dw\n",
    "\n",
    "def train_svm(X, y, C, initial_eta, epochs, batch_size):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    n_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "    \n",
    "    # Initialize gradient accumulation variable\n",
    "    G = np.zeros(X.shape[1])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            grad = hinge_loss_gradient(w, X_batch, y_batch, C)\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            G += grad ** 2\n",
    "            \n",
    "            # Adapt learning rate\n",
    "            eta = initial_eta / np.sqrt(G + 1e-8)\n",
    "            \n",
    "            # Update rule with Adagrad\n",
    "            w -= eta * grad\n",
    "        \n",
    "        # Logging progress\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {hinge_loss(w, X, y, C)}\")\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Parameters\n",
    "file_path = 'toydata_tiny.csv'  # Replace with your actual file path\n",
    "C = 0.01  # Regularization parameter\n",
    "initial_eta = 1.1  # Initial learning rate\n",
    "epochs = 5\n",
    "batch_size = 5\n",
    "\n",
    "# Load and train SVM\n",
    "X, y = load_dataset(file_path)\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "\n",
    "def evaluate_accuracy(X, y, w):\n",
    "    # Get predictions\n",
    "    predictions = predict(X, w)\n",
    "    # Compare predictions with actual labels\n",
    "    correct_predictions = np.sum(predictions == y)\n",
    "    total_predictions = len(y)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy * 100  # return as percentage\n",
    "# Define different sets of parameters to test\n",
    "parameters = [\n",
    "    {'C': 0.01, 'initial_eta': 0.1, 'epochs': 2, 'batch_size': 10},\n",
    "    {'C': 0.1, 'initial_eta': 0.01, 'epochs': 5, 'batch_size': 500},\n",
    "    {'C': 0.5, 'initial_eta': 0.005, 'epochs': 2, 'batch_size': 200},\n",
    "    {'C': 1.0, 'initial_eta': 0.001, 'epochs': 7, 'batch_size': 50},\n",
    "    {'C': 2.0, 'initial_eta': 0.0005, 'epochs': 2, 'batch_size': 100},\n",
    "    {'C': 0.05, 'initial_eta': 0.05, 'epochs': 3, 'batch_size': 30},\n",
    "    {'C': 3.0, 'initial_eta': 0.002, 'epochs': 5, 'batch_size': 75},\n",
    "    {'C': 5.0, 'initial_eta': 0.001, 'epochs': 1, 'batch_size': 250},\n",
    "    {'C': 10.0, 'initial_eta': 0.0001, 'epochs': 3, 'batch_size': 400},\n",
    "    {'C': 0.2, 'initial_eta': 0.01, 'epochs': 5, 'batch_size': 25}\n",
    "]\n",
    "\n",
    "# Test each parameter configuration\n",
    "results = []\n",
    "for params in parameters:\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "    w = train_svm(X_train, y_train, params['C'], params['initial_eta'], params['epochs'], params['batch_size'])\n",
    "    accuracy = evaluate_accuracy(X_test, y_test, w)\n",
    "    results.append((params, accuracy))\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Display results\n",
    "for params, accuracy in results:\n",
    "    print(f\"Parameters: {params}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with C=0.01, eta=0.01, 5 RFFs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (10,5) () ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_rffs \u001b[38;5;129;01min\u001b[39;00m n_rffs_list:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, eta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_rffs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RFFs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_svm_rff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rffs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Apply RFF approximation to test data\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     X_test_rff \u001b[38;5;241m=\u001b[39m rff_kernel(X_test, trained_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m], trained_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mtrain_svm_rff\u001b[1;34m(X, y, C, eta, epochs, batch_size, n_rffs)\u001b[0m\n\u001b[0;32m     41\u001b[0m         X_batch \u001b[38;5;241m=\u001b[39m X_rff[start:end]\n\u001b[0;32m     42\u001b[0m         y_batch \u001b[38;5;241m=\u001b[39m y[start:end]\n\u001b[1;32m---> 43\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[43mhinge_loss_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m         w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m eta \u001b[38;5;241m*\u001b[39m grad\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m: W, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: b}\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mhinge_loss_gradient\u001b[1;34m(w, X, y, C)\u001b[0m\n\u001b[0;32m     23\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, w)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Ensure y is reshaped correctly for broadcasting with X\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# We use `y[:, np.newaxis]` to make y a column vector so it can broadcast with X\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m dw \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m C \u001b[38;5;241m+\u001b[39m w\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dw\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (10,5) () "
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    return X, y\n",
    "\n",
    "def rff_kernel(X, W, b):\n",
    "    return np.sqrt(2. / W.shape[1]) * np.cos(np.dot(X, W.T) + b)\n",
    "\n",
    "def hinge_loss(w, X, y, C):\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    distances[distances < 0] = 0\n",
    "    hinge_loss = C * np.mean(distances) + 0.5 * np.sum(w * w)\n",
    "    return hinge_loss\n",
    "\n",
    "def hinge_loss_gradient(w, X, y, C):\n",
    "    distances = 1 - y * np.dot(X, w)\n",
    "    # Ensure y is reshaped correctly for broadcasting with X\n",
    "    # We use `y[:, np.newaxis]` to make y a column vector so it can broadcast with X\n",
    "    dw = np.where(distances > 0, -y[:, np.newaxis] * X, 0).mean(axis=0) * C + w\n",
    "    return dw\n",
    "\n",
    "def train_svm_rff(X, y, C, eta, epochs, batch_size, n_rffs):\n",
    "    n_samples, n_features = X.shape\n",
    "    W = np.random.normal(0, 1, size=(n_rffs, n_features))\n",
    "    b = np.random.uniform(0, 2 * np.pi, size=n_rffs)\n",
    "    \n",
    "    X_rff = rff_kernel(X, W, b)\n",
    "    w = np.zeros(X_rff.shape[1])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        X_rff, y = shuffle(X_rff, y)  # Shuffle the data to ensure randomness in mini-batch selection\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_rff[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            grad = hinge_loss_gradient(w, X_batch, y_batch, C)\n",
    "            w -= eta * grad\n",
    "    \n",
    "    return {'weights': w, 'W': W, 'b': b}\n",
    "\n",
    "# Parameters\n",
    "file_path = 'toydata_tiny.csv'  # Replace with your actual file path\n",
    "C_values = [0.01, 0.1, 1.0]  # Regularization parameter\n",
    "eta_values = [0.01, 0.1, 1.0]  # Learning rate\n",
    "epochs = 5\n",
    "batch_size = 10\n",
    "n_rffs_list = [5,10,50,100, 200, 300]  # Different numbers of RFFs\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_dataset(file_path)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = 0\n",
    "    total = len(y_true)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "# Test each combination of parameters\n",
    "results = []\n",
    "for C in C_values:\n",
    "    for eta in eta_values:\n",
    "        for n_rffs in n_rffs_list:\n",
    "            print(f\"Training with C={C}, eta={eta}, {n_rffs} RFFs\")\n",
    "            trained_model = train_svm_rff(X_train, y_train, C, eta, epochs, batch_size, n_rffs)\n",
    "            # Apply RFF approximation to test data\n",
    "            X_test_rff = rff_kernel(X_test, trained_model['W'], trained_model['b'])\n",
    "            predictions = np.dot(X_test_rff, trained_model['weights'])\n",
    "            accuracy = accuracy_score(y_test, np.sign(predictions))\n",
    "            results.append((C, eta, n_rffs, accuracy))\n",
    "            print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Display results\n",
    "for C, eta, n_rffs, accuracy in results:\n",
    "    print(f\"C={C}, eta={eta}, {n_rffs} RFFs - Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 100 RFFs: 85.00%\n",
      "Accuracy with 300 RFFs: 86.67%\n",
      "Accuracy with 500 RFFs: 85.00%\n",
      "Accuracy with 750 RFFs: 88.33%\n",
      "Accuracy with 1000 RFFs: 86.67%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std[np.isclose(std, 0)] = 1  # Avoid division by zero for zero-variance features\n",
    "    X = (X - mean) / std\n",
    "    return X, y\n",
    "\n",
    "def hinge_loss_gradient(w, X, y, C):\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    indicator = distances > 0\n",
    "    dw = np.dot(-y[indicator], X[indicator]) / len(y) * C + w\n",
    "    return dw\n",
    "\n",
    "def train_svm(X, y, C, eta, epochs, batch_size):\n",
    "    w = np.random.randn(X.shape[1]) * 0.01\n",
    "    n_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "    for epoch in range(epochs):\n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(start + batch_size, X.shape[0])\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            grad = hinge_loss_gradient(w, X_batch, y_batch, C)\n",
    "            w -= eta * grad\n",
    "    return w\n",
    "\n",
    "def random_fourier_features(X, omega, b):\n",
    "    X_features = np.sqrt(2 / omega.shape[0]) * np.cos(np.dot(X, omega.T) + b)\n",
    "    return X_features\n",
    "\n",
    "def prepare_rff(X_train, X_test, n_features, gamma=1.0):\n",
    "    D = X_train.shape[1]\n",
    "    omega = np.sqrt(2 * gamma) * np.random.randn(n_features, D)\n",
    "    b = np.random.uniform(0, 2 * np.pi, n_features)\n",
    "    X_train_rff = random_fourier_features(X_train, omega, b)\n",
    "    X_test_rff = random_fourier_features(X_test, omega, b)\n",
    "    return X_train_rff, X_test_rff\n",
    "\n",
    "file_path = 'toydata_tiny.csv'\n",
    "X, y = load_dataset(file_path)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "n_rff_values = [100, 300, 500, 750, 1000]\n",
    "\n",
    "for n_rff in n_rff_values:\n",
    "    X_train_rff, X_test_rff = prepare_rff(X_train, X_test, n_rff)\n",
    "    C = 0.01\n",
    "    eta = 1.1\n",
    "    epochs = 5\n",
    "    batch_size = 5\n",
    "    w = train_svm(X_train_rff, y_train, C, eta, epochs, batch_size)\n",
    "    accuracy = evaluate_accuracy(X_test_rff, y_test, w)\n",
    "    print(f\"Accuracy with {n_rff} RFFs: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
